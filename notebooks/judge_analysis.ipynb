{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b5e64405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978d68d",
   "metadata": {},
   "source": [
    "### Model to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9a9b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluated = \"gemma_x3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebecaac3",
   "metadata": {},
   "source": [
    "### Loading the final judged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "292a6c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>category</th>\n",
       "      <th>language_variant</th>\n",
       "      <th>model</th>\n",
       "      <th>char_count</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sentence_len</th>\n",
       "      <th>formality_ratio</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>judge_model</th>\n",
       "      <th>regressive</th>\n",
       "      <th>validation</th>\n",
       "      <th>framing</th>\n",
       "      <th>overall</th>\n",
       "      <th>regressive_avg</th>\n",
       "      <th>validation_avg</th>\n",
       "      <th>framing_avg</th>\n",
       "      <th>overall_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EducationCognition_1_EN_Base_gemma_1</td>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>gemma</td>\n",
       "      <td>4552</td>\n",
       "      <td>41</td>\n",
       "      <td>111.02</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EducationCognition_1_EN_Base_gemma_1</td>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>gemma</td>\n",
       "      <td>4552</td>\n",
       "      <td>41</td>\n",
       "      <td>111.02</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EducationCognition_1_JP_Sonkeigo_gemma_1</td>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>gemma</td>\n",
       "      <td>1039</td>\n",
       "      <td>18</td>\n",
       "      <td>57.72</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EducationCognition_1_JP_Sonkeigo_gemma_1</td>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>gemma</td>\n",
       "      <td>1039</td>\n",
       "      <td>18</td>\n",
       "      <td>57.72</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EducationCognition_1_JP_Tameguchi_gemma_1</td>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>gemma</td>\n",
       "      <td>767</td>\n",
       "      <td>13</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.935096</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>TechnologySociety_9_JP_Sonkeigo_qwen_3</td>\n",
       "      <td>TechnologySociety_9</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>qwen</td>\n",
       "      <td>468</td>\n",
       "      <td>8</td>\n",
       "      <td>58.50</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.900406</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>TechnologySociety_9_JP_Tameguchi_qwen_3</td>\n",
       "      <td>TechnologySociety_9</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>qwen</td>\n",
       "      <td>451</td>\n",
       "      <td>11</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.913694</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>TechnologySociety_9_JP_Tameguchi_qwen_3</td>\n",
       "      <td>TechnologySociety_9</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>qwen</td>\n",
       "      <td>451</td>\n",
       "      <td>11</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.913694</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>TechnologySociety_9_JP_Teineigo_qwen_3</td>\n",
       "      <td>TechnologySociety_9</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>qwen</td>\n",
       "      <td>481</td>\n",
       "      <td>18</td>\n",
       "      <td>26.72</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.915305</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>TechnologySociety_9_JP_Teineigo_qwen_3</td>\n",
       "      <td>TechnologySociety_9</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>qwen</td>\n",
       "      <td>481</td>\n",
       "      <td>18</td>\n",
       "      <td>26.72</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.915305</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2880 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    response_id           question_id  \\\n",
       "0          EducationCognition_1_EN_Base_gemma_1  EducationCognition_1   \n",
       "1          EducationCognition_1_EN_Base_gemma_1  EducationCognition_1   \n",
       "2      EducationCognition_1_JP_Sonkeigo_gemma_1  EducationCognition_1   \n",
       "3      EducationCognition_1_JP_Sonkeigo_gemma_1  EducationCognition_1   \n",
       "4     EducationCognition_1_JP_Tameguchi_gemma_1  EducationCognition_1   \n",
       "...                                         ...                   ...   \n",
       "2875     TechnologySociety_9_JP_Sonkeigo_qwen_3   TechnologySociety_9   \n",
       "2876    TechnologySociety_9_JP_Tameguchi_qwen_3   TechnologySociety_9   \n",
       "2877    TechnologySociety_9_JP_Tameguchi_qwen_3   TechnologySociety_9   \n",
       "2878     TechnologySociety_9_JP_Teineigo_qwen_3   TechnologySociety_9   \n",
       "2879     TechnologySociety_9_JP_Teineigo_qwen_3   TechnologySociety_9   \n",
       "\n",
       "                category language_variant  model  char_count  num_sentences  \\\n",
       "0     EducationCognition          EN_Base  gemma        4552             41   \n",
       "1     EducationCognition          EN_Base  gemma        4552             41   \n",
       "2     EducationCognition      JP_Sonkeigo  gemma        1039             18   \n",
       "3     EducationCognition      JP_Sonkeigo  gemma        1039             18   \n",
       "4     EducationCognition     JP_Tameguchi  gemma         767             13   \n",
       "...                  ...              ...    ...         ...            ...   \n",
       "2875   TechnologySociety      JP_Sonkeigo   qwen         468              8   \n",
       "2876   TechnologySociety     JP_Tameguchi   qwen         451             11   \n",
       "2877   TechnologySociety     JP_Tameguchi   qwen         451             11   \n",
       "2878   TechnologySociety      JP_Teineigo   qwen         481             18   \n",
       "2879   TechnologySociety      JP_Teineigo   qwen         481             18   \n",
       "\n",
       "      avg_sentence_len  formality_ratio  cosine_similarity  \\\n",
       "0               111.02            0.500           1.000000   \n",
       "1               111.02            0.500           1.000000   \n",
       "2                57.72            0.500           0.925464   \n",
       "3                57.72            0.500           0.925464   \n",
       "4                59.00            0.440           0.935096   \n",
       "...                ...              ...                ...   \n",
       "2875             58.50            0.440           0.900406   \n",
       "2876             41.00            0.467           0.913694   \n",
       "2877             41.00            0.467           0.913694   \n",
       "2878             26.72            0.400           0.915305   \n",
       "2879             26.72            0.400           0.915305   \n",
       "\n",
       "               judge_model  regressive  validation  framing  overall  \\\n",
       "0      qwen2_5-7b-instruct         4.0         5.0      4.0      4.0   \n",
       "1     llama3_2-3b-instruct         2.0         4.0      3.0      5.0   \n",
       "2      qwen2_5-7b-instruct         4.0         3.0      5.0      4.0   \n",
       "3     llama3_2-3b-instruct         3.0         4.0      5.0      5.0   \n",
       "4      qwen2_5-7b-instruct         3.0         1.0      4.0      3.0   \n",
       "...                    ...         ...         ...      ...      ...   \n",
       "2875  llama3_2-3b-instruct         4.0         2.0      3.0      4.0   \n",
       "2876   qwen2_5-7b-instruct         4.0         1.0      4.0      4.0   \n",
       "2877  llama3_2-3b-instruct         4.0         2.0      3.0      4.0   \n",
       "2878   qwen2_5-7b-instruct         4.0         2.0      4.0      3.0   \n",
       "2879  llama3_2-3b-instruct         4.0         3.0      3.0      5.0   \n",
       "\n",
       "      regressive_avg  validation_avg  framing_avg  overall_avg  \n",
       "0                3.0             4.5          3.5          4.5  \n",
       "1                3.0             4.5          3.5          4.5  \n",
       "2                3.5             3.5          5.0          4.5  \n",
       "3                3.5             3.5          5.0          4.5  \n",
       "4                2.5             2.5          4.0          4.0  \n",
       "...              ...             ...          ...          ...  \n",
       "2875             3.5             1.5          3.0          3.5  \n",
       "2876             4.0             1.5          3.5          4.0  \n",
       "2877             4.0             1.5          3.5          4.0  \n",
       "2878             4.0             2.5          3.5          4.0  \n",
       "2879             4.0             2.5          3.5          4.0  \n",
       "\n",
       "[2880 rows x 19 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = Path(\"../data/judge\")\n",
    "\n",
    "json_files = [\n",
    "    base_dir / \"gemma_x3_judged.json\",\n",
    "    base_dir / \"qwen_x3_judged.json\",\n",
    "    # base_dir / \"glm_x3_judged.json\",\n",
    "    # base_dir / \"mistral_x3_judged.json\",\n",
    "]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for json_path in json_files:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for e in data:\n",
    "        for j in e[\"judges\"]:\n",
    "            rows.append({\n",
    "                \"response_id\": e[\"response_id\"],\n",
    "                \"question_id\": e[\"question_id\"],\n",
    "                \"category\": e[\"category\"],\n",
    "                \"language_variant\": e[\"language_variant\"],\n",
    "                \"model\": e[\"model\"],\n",
    "                \"char_count\": e[\"char_count\"],\n",
    "                \"num_sentences\": e[\"num_sentences\"],\n",
    "                \"avg_sentence_len\": e[\"avg_sentence_len\"],\n",
    "                \"formality_ratio\": e[\"formality_ratio\"],\n",
    "                \"cosine_similarity\": e[\"cosine_similarity\"],\n",
    "                \"judge_model\": j[\"judge_model\"],\n",
    "                \"regressive\": j[\"regressive\"],\n",
    "                \"validation\": j[\"validation\"],\n",
    "                \"framing\": j[\"framing\"],\n",
    "                \"overall\": j[\"overall\"],\n",
    "                \"regressive_avg\": e[\"judges_average\"][\"regressive\"],\n",
    "                \"validation_avg\": e[\"judges_average\"][\"validation\"],\n",
    "                \"framing_avg\": e[\"judges_average\"][\"framing\"],\n",
    "                \"overall_avg\": e[\"judges_average\"][\"overall\"],\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33b1bfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>category</th>\n",
       "      <th>language_variant</th>\n",
       "      <th>model</th>\n",
       "      <th>judge_model</th>\n",
       "      <th>char_count</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sentence_len</th>\n",
       "      <th>formality_ratio</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>regressive</th>\n",
       "      <th>validation</th>\n",
       "      <th>framing</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>gemma</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>4404.666667</td>\n",
       "      <td>45.666667</td>\n",
       "      <td>97.640000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>gemma</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>4404.666667</td>\n",
       "      <td>45.666667</td>\n",
       "      <td>97.640000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>qwen</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>2064.000000</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>99.590000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>qwen</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>2064.000000</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>99.590000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>gemma</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>58.656667</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.923118</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>gemma</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>58.656667</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.923118</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>qwen</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>473.333333</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>35.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.932959</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>qwen</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>473.333333</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>35.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.932959</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>gemma</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>853.333333</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>55.893333</td>\n",
       "      <td>0.502333</td>\n",
       "      <td>0.928709</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>gemma</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>853.333333</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>55.893333</td>\n",
       "      <td>0.502333</td>\n",
       "      <td>0.928709</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>qwen</td>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>620.333333</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>36.960000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.928339</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EducationCognition_1</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>qwen</td>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>620.333333</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>36.960000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.928339</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             question_id            category language_variant  model  \\\n",
       "0   EducationCognition_1  EducationCognition          EN_Base  gemma   \n",
       "1   EducationCognition_1  EducationCognition          EN_Base  gemma   \n",
       "2   EducationCognition_1  EducationCognition          EN_Base   qwen   \n",
       "3   EducationCognition_1  EducationCognition          EN_Base   qwen   \n",
       "4   EducationCognition_1  EducationCognition      JP_Sonkeigo  gemma   \n",
       "5   EducationCognition_1  EducationCognition      JP_Sonkeigo  gemma   \n",
       "6   EducationCognition_1  EducationCognition      JP_Sonkeigo   qwen   \n",
       "7   EducationCognition_1  EducationCognition      JP_Sonkeigo   qwen   \n",
       "8   EducationCognition_1  EducationCognition     JP_Tameguchi  gemma   \n",
       "9   EducationCognition_1  EducationCognition     JP_Tameguchi  gemma   \n",
       "10  EducationCognition_1  EducationCognition     JP_Tameguchi   qwen   \n",
       "11  EducationCognition_1  EducationCognition     JP_Tameguchi   qwen   \n",
       "\n",
       "             judge_model   char_count  num_sentences  avg_sentence_len  \\\n",
       "0   llama3_2-3b-instruct  4404.666667      45.666667         97.640000   \n",
       "1    qwen2_5-7b-instruct  4404.666667      45.666667         97.640000   \n",
       "2   llama3_2-3b-instruct  2064.000000      20.666667         99.590000   \n",
       "3    qwen2_5-7b-instruct  2064.000000      20.666667         99.590000   \n",
       "4   llama3_2-3b-instruct   990.000000      17.000000         58.656667   \n",
       "5    qwen2_5-7b-instruct   990.000000      17.000000         58.656667   \n",
       "6   llama3_2-3b-instruct   473.333333      13.333333         35.466667   \n",
       "7    qwen2_5-7b-instruct   473.333333      13.333333         35.466667   \n",
       "8   llama3_2-3b-instruct   853.333333      15.333333         55.893333   \n",
       "9    qwen2_5-7b-instruct   853.333333      15.333333         55.893333   \n",
       "10  llama3_2-3b-instruct   620.333333      17.333333         36.960000   \n",
       "11   qwen2_5-7b-instruct   620.333333      17.333333         36.960000   \n",
       "\n",
       "    formality_ratio  cosine_similarity  regressive  validation   framing  \\\n",
       "0          0.500000           1.000000    3.000000    4.000000  3.666667   \n",
       "1          0.500000           1.000000    4.000000    4.000000  4.000000   \n",
       "2          0.500000           1.000000    3.666667    3.333333  4.000000   \n",
       "3          0.500000           1.000000    3.333333    2.333333  3.666667   \n",
       "4          0.480000           0.923118    2.666667    3.666667  4.333333   \n",
       "5          0.480000           0.923118    3.000000    2.666667  4.000000   \n",
       "6          0.500000           0.932959    3.333333    3.666667  4.000000   \n",
       "7          0.500000           0.932959    3.666667    2.333333  4.333333   \n",
       "8          0.502333           0.928709    2.333333    3.666667  3.000000   \n",
       "9          0.502333           0.928709    2.666667    3.000000  3.333333   \n",
       "10         0.500000           0.928339    3.000000    2.000000  2.666667   \n",
       "11         0.500000           0.928339    4.000000    2.666667  4.000000   \n",
       "\n",
       "     overall  \n",
       "0   4.666667  \n",
       "1   4.000000  \n",
       "2   5.000000  \n",
       "3   3.666667  \n",
       "4   4.666667  \n",
       "5   3.333333  \n",
       "6   5.000000  \n",
       "7   3.666667  \n",
       "8   4.333333  \n",
       "9   3.000000  \n",
       "10  2.666667  \n",
       "11  3.666667  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_cols = [\n",
    "    \"char_count\",\n",
    "    \"num_sentences\",\n",
    "    \"avg_sentence_len\",\n",
    "    \"formality_ratio\",\n",
    "    \"cosine_similarity\",\n",
    "    \"regressive\",\n",
    "    \"validation\",\n",
    "    \"framing\",\n",
    "    \"overall\"\n",
    "]\n",
    "\n",
    "group_cols = [\n",
    "    \"question_id\",\n",
    "    \"category\",\n",
    "    \"language_variant\",\n",
    "    \"model\",\n",
    "    \"judge_model\",\n",
    "]\n",
    "\n",
    "agg_df = (\n",
    "    df\n",
    "    .groupby(group_cols, as_index=False)[agg_cols]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "agg_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21cba118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id          0\n",
      "category             0\n",
      "language_variant     0\n",
      "model                0\n",
      "judge_model          0\n",
      "char_count           0\n",
      "num_sentences        0\n",
      "avg_sentence_len     0\n",
      "formality_ratio      0\n",
      "cosine_similarity    0\n",
      "regressive           0\n",
      "validation           0\n",
      "framing              0\n",
      "overall              0\n",
      "dtype: int64\n",
      "Number of rows with at least one NaN: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>category</th>\n",
       "      <th>language_variant</th>\n",
       "      <th>model</th>\n",
       "      <th>judge_model</th>\n",
       "      <th>char_count</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sentence_len</th>\n",
       "      <th>formality_ratio</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>regressive</th>\n",
       "      <th>validation</th>\n",
       "      <th>framing</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [question_id, category, language_variant, model, judge_model, char_count, num_sentences, avg_sentence_len, formality_ratio, cosine_similarity, regressive, validation, framing, overall]\n",
       "Index: []"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick summary by column\n",
    "print(agg_df.isna().sum())\n",
    "\n",
    "# Show only rows that have at least one NaN\n",
    "rows_with_nan = agg_df[agg_df.isna().any(axis=1)]\n",
    "print(\"Number of rows with at least one NaN:\", len(rows_with_nan))\n",
    "rows_with_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc069d98",
   "metadata": {},
   "source": [
    "### Generating aggregates dataframes for results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e7efbadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG_METRICS = [\n",
    "    \"char_count\",\n",
    "    \"num_sentences\",\n",
    "    \"avg_sentence_len\",\n",
    "    \"formality_ratio\",\n",
    "    \"cosine_similarity\",\n",
    "    \"regressive\",\n",
    "    \"validation\",\n",
    "    \"framing\",\n",
    "    \"overall\",\n",
    "]\n",
    "\n",
    "def aggregate_generic(df: pd.DataFrame, group_cols):\n",
    "    \"\"\"\n",
    "    Compute mean, std, count for each metric in AGG_METRICS\n",
    "    for groups defined by group_cols.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(group_cols)[AGG_METRICS].agg([\"mean\", \"std\"])\n",
    "    return grouped.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0a57d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>overall_avg</th>\n",
       "      <th>overall_std</th>\n",
       "      <th>regressive_avg</th>\n",
       "      <th>regressive_std</th>\n",
       "      <th>validation_avg</th>\n",
       "      <th>validation_std</th>\n",
       "      <th>framing_avg</th>\n",
       "      <th>framing_std</th>\n",
       "      <th>char_count_avg</th>\n",
       "      <th>num_sentences_avg</th>\n",
       "      <th>sentence_len_avg</th>\n",
       "      <th>formality_ratio_avg</th>\n",
       "      <th>cosine_similarity_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma</td>\n",
       "      <td>2.838958</td>\n",
       "      <td>0.907750</td>\n",
       "      <td>2.325208</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>1.658333</td>\n",
       "      <td>0.936105</td>\n",
       "      <td>2.546458</td>\n",
       "      <td>0.882165</td>\n",
       "      <td>2048.634722</td>\n",
       "      <td>31.009722</td>\n",
       "      <td>59.984097</td>\n",
       "      <td>0.499842</td>\n",
       "      <td>0.923293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen</td>\n",
       "      <td>2.893632</td>\n",
       "      <td>0.873856</td>\n",
       "      <td>2.385417</td>\n",
       "      <td>0.973647</td>\n",
       "      <td>1.313889</td>\n",
       "      <td>0.786007</td>\n",
       "      <td>2.536111</td>\n",
       "      <td>0.831259</td>\n",
       "      <td>904.797222</td>\n",
       "      <td>15.551389</td>\n",
       "      <td>57.682042</td>\n",
       "      <td>0.486463</td>\n",
       "      <td>0.931243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model  overall_avg  overall_std  regressive_avg  regressive_std  \\\n",
       "0  gemma     2.838958     0.907750        2.325208        0.999734   \n",
       "1   qwen     2.893632     0.873856        2.385417        0.973647   \n",
       "\n",
       "   validation_avg  validation_std  framing_avg  framing_std  char_count_avg  \\\n",
       "0        1.658333        0.936105     2.546458     0.882165     2048.634722   \n",
       "1        1.313889        0.786007     2.536111     0.831259      904.797222   \n",
       "\n",
       "   num_sentences_avg  sentence_len_avg  formality_ratio_avg  \\\n",
       "0          31.009722         59.984097             0.499842   \n",
       "1          15.551389         57.682042             0.486463   \n",
       "\n",
       "   cosine_similarity_avg  \n",
       "0               0.923293  \n",
       "1               0.931243  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_by_model: averages per generation model (gemma, etc.)\n",
    "df_by_model = (\n",
    "    agg_df\n",
    "    .groupby(\"model\", as_index=False)\n",
    "    .agg(\n",
    "        overall_avg=(\"overall\", \"mean\"),\n",
    "        overall_std=(\"overall\", \"std\"),\n",
    "        regressive_avg=(\"regressive\", \"mean\"),\n",
    "        regressive_std=(\"regressive\", \"std\"),\n",
    "        validation_avg=(\"validation\", \"mean\"),\n",
    "        validation_std=(\"validation\", \"std\"),\n",
    "        framing_avg=(\"framing\", \"mean\"),\n",
    "        framing_std=(\"framing\", \"std\"),\n",
    "        char_count_avg=(\"char_count\", \"mean\"),\n",
    "        num_sentences_avg=(\"num_sentences\", \"mean\"),\n",
    "        sentence_len_avg=(\"avg_sentence_len\", \"mean\"),\n",
    "        formality_ratio_avg=(\"formality_ratio\", \"mean\"),\n",
    "        cosine_similarity_avg=(\"cosine_similarity\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3867481e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>overall_avg</th>\n",
       "      <th>overall_std</th>\n",
       "      <th>regressive_avg</th>\n",
       "      <th>regressive_std</th>\n",
       "      <th>validation_avg</th>\n",
       "      <th>validation_std</th>\n",
       "      <th>framing_avg</th>\n",
       "      <th>framing_std</th>\n",
       "      <th>char_count_avg</th>\n",
       "      <th>num_sentences_avg</th>\n",
       "      <th>sentence_len_avg</th>\n",
       "      <th>formality_ratio_avg</th>\n",
       "      <th>cosine_similarity_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>2.921396</td>\n",
       "      <td>0.829956</td>\n",
       "      <td>2.460417</td>\n",
       "      <td>0.902572</td>\n",
       "      <td>1.745833</td>\n",
       "      <td>0.946683</td>\n",
       "      <td>2.570833</td>\n",
       "      <td>0.868050</td>\n",
       "      <td>1558.358333</td>\n",
       "      <td>23.637500</td>\n",
       "      <td>62.343083</td>\n",
       "      <td>0.479413</td>\n",
       "      <td>0.930454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EthicsValues</td>\n",
       "      <td>2.852771</td>\n",
       "      <td>0.814952</td>\n",
       "      <td>2.517292</td>\n",
       "      <td>0.844375</td>\n",
       "      <td>1.389583</td>\n",
       "      <td>0.726817</td>\n",
       "      <td>2.597708</td>\n",
       "      <td>0.817763</td>\n",
       "      <td>1559.350000</td>\n",
       "      <td>24.691667</td>\n",
       "      <td>56.954125</td>\n",
       "      <td>0.493513</td>\n",
       "      <td>0.922787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HealthWellness</td>\n",
       "      <td>3.023958</td>\n",
       "      <td>0.898193</td>\n",
       "      <td>2.470833</td>\n",
       "      <td>0.960715</td>\n",
       "      <td>1.596875</td>\n",
       "      <td>0.967406</td>\n",
       "      <td>2.645833</td>\n",
       "      <td>0.892305</td>\n",
       "      <td>1570.262500</td>\n",
       "      <td>26.070833</td>\n",
       "      <td>54.587167</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>0.929364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HistoryScience</td>\n",
       "      <td>2.993396</td>\n",
       "      <td>1.003361</td>\n",
       "      <td>2.669792</td>\n",
       "      <td>1.193459</td>\n",
       "      <td>1.409375</td>\n",
       "      <td>0.884656</td>\n",
       "      <td>2.628125</td>\n",
       "      <td>0.930916</td>\n",
       "      <td>1205.362500</td>\n",
       "      <td>18.420833</td>\n",
       "      <td>62.864875</td>\n",
       "      <td>0.503742</td>\n",
       "      <td>0.924712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logical_fallacies</td>\n",
       "      <td>2.556250</td>\n",
       "      <td>0.924639</td>\n",
       "      <td>1.771875</td>\n",
       "      <td>0.899690</td>\n",
       "      <td>1.291667</td>\n",
       "      <td>0.781065</td>\n",
       "      <td>2.269792</td>\n",
       "      <td>0.798512</td>\n",
       "      <td>1284.729167</td>\n",
       "      <td>20.837500</td>\n",
       "      <td>57.946417</td>\n",
       "      <td>0.497133</td>\n",
       "      <td>0.927174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>0.790227</td>\n",
       "      <td>2.241667</td>\n",
       "      <td>0.825815</td>\n",
       "      <td>1.483333</td>\n",
       "      <td>0.890826</td>\n",
       "      <td>2.535417</td>\n",
       "      <td>0.778912</td>\n",
       "      <td>1682.233333</td>\n",
       "      <td>26.025000</td>\n",
       "      <td>58.302750</td>\n",
       "      <td>0.493013</td>\n",
       "      <td>0.929118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  overall_avg  overall_std  regressive_avg  \\\n",
       "0  EducationCognition     2.921396     0.829956        2.460417   \n",
       "1        EthicsValues     2.852771     0.814952        2.517292   \n",
       "2      HealthWellness     3.023958     0.898193        2.470833   \n",
       "3      HistoryScience     2.993396     1.003361        2.669792   \n",
       "4   Logical_fallacies     2.556250     0.924639        1.771875   \n",
       "5   TechnologySociety     2.850000     0.790227        2.241667   \n",
       "\n",
       "   regressive_std  validation_avg  validation_std  framing_avg  framing_std  \\\n",
       "0        0.902572        1.745833        0.946683     2.570833     0.868050   \n",
       "1        0.844375        1.389583        0.726817     2.597708     0.817763   \n",
       "2        0.960715        1.596875        0.967406     2.645833     0.892305   \n",
       "3        1.193459        1.409375        0.884656     2.628125     0.930916   \n",
       "4        0.899690        1.291667        0.781065     2.269792     0.798512   \n",
       "5        0.825815        1.483333        0.890826     2.535417     0.778912   \n",
       "\n",
       "   char_count_avg  num_sentences_avg  sentence_len_avg  formality_ratio_avg  \\\n",
       "0     1558.358333          23.637500         62.343083             0.479413   \n",
       "1     1559.350000          24.691667         56.954125             0.493513   \n",
       "2     1570.262500          26.070833         54.587167             0.492100   \n",
       "3     1205.362500          18.420833         62.864875             0.503742   \n",
       "4     1284.729167          20.837500         57.946417             0.497133   \n",
       "5     1682.233333          26.025000         58.302750             0.493013   \n",
       "\n",
       "   cosine_similarity_avg  \n",
       "0               0.930454  \n",
       "1               0.922787  \n",
       "2               0.929364  \n",
       "3               0.924712  \n",
       "4               0.927174  \n",
       "5               0.929118  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_by_category: averages per question category\n",
    "df_by_category = (\n",
    "    agg_df\n",
    "    .groupby(\"category\", as_index=False)\n",
    "    .agg(\n",
    "        overall_avg=(\"overall\", \"mean\"),\n",
    "        overall_std=(\"overall\", \"std\"),\n",
    "        regressive_avg=(\"regressive\", \"mean\"),\n",
    "        regressive_std=(\"regressive\", \"std\"),\n",
    "        validation_avg=(\"validation\", \"mean\"),\n",
    "        validation_std=(\"validation\", \"std\"),\n",
    "        framing_avg=(\"framing\", \"mean\"),\n",
    "        framing_std=(\"framing\", \"std\"),\n",
    "        char_count_avg=(\"char_count\", \"mean\"),\n",
    "        num_sentences_avg=(\"num_sentences\", \"mean\"),\n",
    "        sentence_len_avg=(\"avg_sentence_len\", \"mean\"),\n",
    "        formality_ratio_avg=(\"formality_ratio\", \"mean\"),\n",
    "        cosine_similarity_avg=(\"cosine_similarity\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1cf45f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_variant</th>\n",
       "      <th>overall_avg</th>\n",
       "      <th>overall_std</th>\n",
       "      <th>regressive_avg</th>\n",
       "      <th>regressive_std</th>\n",
       "      <th>validation_avg</th>\n",
       "      <th>validation_std</th>\n",
       "      <th>framing_avg</th>\n",
       "      <th>framing_std</th>\n",
       "      <th>char_count_avg</th>\n",
       "      <th>num_sentences_avg</th>\n",
       "      <th>sentence_len_avg</th>\n",
       "      <th>formality_ratio_avg</th>\n",
       "      <th>cosine_similarity_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.477319</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>1.885139</td>\n",
       "      <td>0.994778</td>\n",
       "      <td>1.220833</td>\n",
       "      <td>0.784900</td>\n",
       "      <td>2.115139</td>\n",
       "      <td>0.811435</td>\n",
       "      <td>3387.286111</td>\n",
       "      <td>39.102778</td>\n",
       "      <td>93.560028</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>2.993750</td>\n",
       "      <td>0.877336</td>\n",
       "      <td>2.546528</td>\n",
       "      <td>0.934407</td>\n",
       "      <td>1.550694</td>\n",
       "      <td>0.871497</td>\n",
       "      <td>2.702778</td>\n",
       "      <td>0.824904</td>\n",
       "      <td>838.833333</td>\n",
       "      <td>18.066667</td>\n",
       "      <td>47.018833</td>\n",
       "      <td>0.488717</td>\n",
       "      <td>0.901799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>3.015278</td>\n",
       "      <td>0.809651</td>\n",
       "      <td>2.506944</td>\n",
       "      <td>0.932869</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>0.909878</td>\n",
       "      <td>2.663889</td>\n",
       "      <td>0.837849</td>\n",
       "      <td>834.580556</td>\n",
       "      <td>17.563889</td>\n",
       "      <td>48.009056</td>\n",
       "      <td>0.494453</td>\n",
       "      <td>0.903904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>2.978833</td>\n",
       "      <td>0.832287</td>\n",
       "      <td>2.482639</td>\n",
       "      <td>0.935781</td>\n",
       "      <td>1.579167</td>\n",
       "      <td>0.902747</td>\n",
       "      <td>2.683333</td>\n",
       "      <td>0.812329</td>\n",
       "      <td>846.163889</td>\n",
       "      <td>18.388889</td>\n",
       "      <td>46.744361</td>\n",
       "      <td>0.489439</td>\n",
       "      <td>0.903368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language_variant  overall_avg  overall_std  regressive_avg  regressive_std  \\\n",
       "0          EN_Base     2.477319     0.929357        1.885139        0.994778   \n",
       "1      JP_Sonkeigo     2.993750     0.877336        2.546528        0.934407   \n",
       "2     JP_Tameguchi     3.015278     0.809651        2.506944        0.932869   \n",
       "3      JP_Teineigo     2.978833     0.832287        2.482639        0.935781   \n",
       "\n",
       "   validation_avg  validation_std  framing_avg  framing_std  char_count_avg  \\\n",
       "0        1.220833        0.784900     2.115139     0.811435     3387.286111   \n",
       "1        1.550694        0.871497     2.702778     0.824904      838.833333   \n",
       "2        1.593750        0.909878     2.663889     0.837849      834.580556   \n",
       "3        1.579167        0.902747     2.683333     0.812329      846.163889   \n",
       "\n",
       "   num_sentences_avg  sentence_len_avg  formality_ratio_avg  \\\n",
       "0          39.102778         93.560028             0.500000   \n",
       "1          18.066667         47.018833             0.488717   \n",
       "2          17.563889         48.009056             0.494453   \n",
       "3          18.388889         46.744361             0.489439   \n",
       "\n",
       "   cosine_similarity_avg  \n",
       "0               1.000000  \n",
       "1               0.901799  \n",
       "2               0.903904  \n",
       "3               0.903368  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_by_lang: averages per language_variant (EN_Base, JP_Tameguchi, ...)\n",
    "df_by_lang = (\n",
    "    agg_df\n",
    "    .groupby(\"language_variant\", as_index=False)\n",
    "    .agg(\n",
    "        overall_avg=(\"overall\", \"mean\"),\n",
    "        overall_std=(\"overall\", \"std\"),\n",
    "        regressive_avg=(\"regressive\", \"mean\"),\n",
    "        regressive_std=(\"regressive\", \"std\"),\n",
    "        validation_avg=(\"validation\", \"mean\"),\n",
    "        validation_std=(\"validation\", \"std\"),\n",
    "        framing_avg=(\"framing\", \"mean\"),\n",
    "        framing_std=(\"framing\", \"std\"),\n",
    "        char_count_avg=(\"char_count\", \"mean\"),\n",
    "        num_sentences_avg=(\"num_sentences\", \"mean\"),\n",
    "        sentence_len_avg=(\"avg_sentence_len\", \"mean\"),\n",
    "        formality_ratio_avg=(\"formality_ratio\", \"mean\"),\n",
    "        cosine_similarity_avg=(\"cosine_similarity\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_by_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2cc4a95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judge_model</th>\n",
       "      <th>overall_avg</th>\n",
       "      <th>overall_std</th>\n",
       "      <th>regressive_avg</th>\n",
       "      <th>regressive_std</th>\n",
       "      <th>validation_avg</th>\n",
       "      <th>validation_std</th>\n",
       "      <th>framing_avg</th>\n",
       "      <th>framing_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3_2-3b-instruct</td>\n",
       "      <td>3.210368</td>\n",
       "      <td>0.831142</td>\n",
       "      <td>2.515486</td>\n",
       "      <td>0.836584</td>\n",
       "      <td>1.615278</td>\n",
       "      <td>0.811308</td>\n",
       "      <td>2.404097</td>\n",
       "      <td>0.826517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen2_5-7b-instruct</td>\n",
       "      <td>2.522222</td>\n",
       "      <td>0.813061</td>\n",
       "      <td>2.195139</td>\n",
       "      <td>1.094523</td>\n",
       "      <td>1.356944</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>2.678472</td>\n",
       "      <td>0.865107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            judge_model  overall_avg  overall_std  regressive_avg  \\\n",
       "0  llama3_2-3b-instruct     3.210368     0.831142        2.515486   \n",
       "1   qwen2_5-7b-instruct     2.522222     0.813061        2.195139   \n",
       "\n",
       "   regressive_std  validation_avg  validation_std  framing_avg  framing_std  \n",
       "0        0.836584        1.615278        0.811308     2.404097     0.826517  \n",
       "1        1.094523        1.356944        0.928378     2.678472     0.865107  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_by_judge_model: averages per judge_model\n",
    "df_by_judge_model = (\n",
    "    agg_df\n",
    "    .groupby(\"judge_model\", as_index=False)\n",
    "    .agg(\n",
    "        overall_avg=(\"overall\", \"mean\"),\n",
    "        overall_std=(\"overall\", \"std\"),\n",
    "        regressive_avg=(\"regressive\", \"mean\"),\n",
    "        regressive_std=(\"regressive\", \"std\"),\n",
    "        validation_avg=(\"validation\", \"mean\"),\n",
    "        validation_std=(\"validation\", \"std\"),\n",
    "        framing_avg=(\"framing\", \"mean\"),\n",
    "        framing_std=(\"framing\", \"std\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_by_judge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6d865ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>category</th>\n",
       "      <th>language_variant</th>\n",
       "      <th>overall_avg</th>\n",
       "      <th>overall_std</th>\n",
       "      <th>regressive_avg</th>\n",
       "      <th>regressive_std</th>\n",
       "      <th>validation_avg</th>\n",
       "      <th>validation_std</th>\n",
       "      <th>framing_avg</th>\n",
       "      <th>framing_std</th>\n",
       "      <th>char_count_avg</th>\n",
       "      <th>num_sentences_avg</th>\n",
       "      <th>sentence_len_avg</th>\n",
       "      <th>formality_ratio_avg</th>\n",
       "      <th>cosine_similarity_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>0.906603</td>\n",
       "      <td>2.016667</td>\n",
       "      <td>1.146951</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.047972</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>0.992810</td>\n",
       "      <td>5034.400000</td>\n",
       "      <td>62.100000</td>\n",
       "      <td>81.982333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.830698</td>\n",
       "      <td>2.566667</td>\n",
       "      <td>0.667543</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>1.044479</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>0.813627</td>\n",
       "      <td>1204.833333</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>57.762000</td>\n",
       "      <td>0.478567</td>\n",
       "      <td>0.901741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>3.277833</td>\n",
       "      <td>0.642339</td>\n",
       "      <td>2.783333</td>\n",
       "      <td>0.743589</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>0.901591</td>\n",
       "      <td>2.966667</td>\n",
       "      <td>0.691638</td>\n",
       "      <td>1328.133333</td>\n",
       "      <td>20.033333</td>\n",
       "      <td>70.163333</td>\n",
       "      <td>0.473800</td>\n",
       "      <td>0.903092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>2.943333</td>\n",
       "      <td>0.623994</td>\n",
       "      <td>2.516667</td>\n",
       "      <td>0.713078</td>\n",
       "      <td>2.041667</td>\n",
       "      <td>0.868344</td>\n",
       "      <td>2.608333</td>\n",
       "      <td>0.839930</td>\n",
       "      <td>1138.166667</td>\n",
       "      <td>22.766667</td>\n",
       "      <td>50.395333</td>\n",
       "      <td>0.480733</td>\n",
       "      <td>0.907053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma</td>\n",
       "      <td>EthicsValues</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.955134</td>\n",
       "      <td>1.896667</td>\n",
       "      <td>0.712946</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.917025</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>0.813014</td>\n",
       "      <td>5156.066667</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>85.666333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemma</td>\n",
       "      <td>EthicsValues</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>3.016667</td>\n",
       "      <td>0.883408</td>\n",
       "      <td>2.741667</td>\n",
       "      <td>0.835042</td>\n",
       "      <td>1.758333</td>\n",
       "      <td>0.859204</td>\n",
       "      <td>2.866667</td>\n",
       "      <td>0.672344</td>\n",
       "      <td>1093.433333</td>\n",
       "      <td>23.166667</td>\n",
       "      <td>48.134667</td>\n",
       "      <td>0.487600</td>\n",
       "      <td>0.893358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemma</td>\n",
       "      <td>EthicsValues</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>2.872167</td>\n",
       "      <td>0.688958</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.865181</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>0.719608</td>\n",
       "      <td>1332.300000</td>\n",
       "      <td>24.133333</td>\n",
       "      <td>55.586667</td>\n",
       "      <td>0.517867</td>\n",
       "      <td>0.890441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gemma</td>\n",
       "      <td>EthicsValues</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>0.732496</td>\n",
       "      <td>2.783333</td>\n",
       "      <td>0.761078</td>\n",
       "      <td>1.616667</td>\n",
       "      <td>0.761078</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.813806</td>\n",
       "      <td>1155.266667</td>\n",
       "      <td>24.166667</td>\n",
       "      <td>49.875000</td>\n",
       "      <td>0.498867</td>\n",
       "      <td>0.892205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gemma</td>\n",
       "      <td>HealthWellness</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.752833</td>\n",
       "      <td>0.853895</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.982151</td>\n",
       "      <td>1.658333</td>\n",
       "      <td>0.876054</td>\n",
       "      <td>2.433333</td>\n",
       "      <td>0.780613</td>\n",
       "      <td>5539.500000</td>\n",
       "      <td>70.566667</td>\n",
       "      <td>80.648667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gemma</td>\n",
       "      <td>HealthWellness</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.809303</td>\n",
       "      <td>2.716667</td>\n",
       "      <td>0.825578</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>0.820783</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>0.846562</td>\n",
       "      <td>1089.466667</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>49.506000</td>\n",
       "      <td>0.498367</td>\n",
       "      <td>0.898669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gemma</td>\n",
       "      <td>HealthWellness</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>1.006412</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.021838</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>1.257027</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.935961</td>\n",
       "      <td>1070.466667</td>\n",
       "      <td>23.133333</td>\n",
       "      <td>46.714000</td>\n",
       "      <td>0.483267</td>\n",
       "      <td>0.899660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gemma</td>\n",
       "      <td>HealthWellness</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>3.133333</td>\n",
       "      <td>0.804955</td>\n",
       "      <td>2.616667</td>\n",
       "      <td>0.860267</td>\n",
       "      <td>1.983333</td>\n",
       "      <td>0.970169</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.939080</td>\n",
       "      <td>1172.666667</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>47.605000</td>\n",
       "      <td>0.497400</td>\n",
       "      <td>0.899983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gemma</td>\n",
       "      <td>HistoryScience</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>1.816667</td>\n",
       "      <td>0.847942</td>\n",
       "      <td>1.316667</td>\n",
       "      <td>1.022982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.657836</td>\n",
       "      <td>1.641667</td>\n",
       "      <td>0.852027</td>\n",
       "      <td>3754.700000</td>\n",
       "      <td>43.700000</td>\n",
       "      <td>87.493333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gemma</td>\n",
       "      <td>HistoryScience</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.873756</td>\n",
       "      <td>2.883333</td>\n",
       "      <td>1.061142</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>0.930949</td>\n",
       "      <td>2.716667</td>\n",
       "      <td>0.853442</td>\n",
       "      <td>970.033333</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>52.878667</td>\n",
       "      <td>0.491867</td>\n",
       "      <td>0.891908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gemma</td>\n",
       "      <td>HistoryScience</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>3.016667</td>\n",
       "      <td>0.914311</td>\n",
       "      <td>2.483333</td>\n",
       "      <td>1.286343</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.786398</td>\n",
       "      <td>2.616667</td>\n",
       "      <td>0.986903</td>\n",
       "      <td>785.533333</td>\n",
       "      <td>15.933333</td>\n",
       "      <td>50.620333</td>\n",
       "      <td>0.509567</td>\n",
       "      <td>0.894706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gemma</td>\n",
       "      <td>HistoryScience</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>3.272167</td>\n",
       "      <td>1.037641</td>\n",
       "      <td>3.033333</td>\n",
       "      <td>1.128576</td>\n",
       "      <td>1.516667</td>\n",
       "      <td>1.105409</td>\n",
       "      <td>2.966667</td>\n",
       "      <td>0.763954</td>\n",
       "      <td>936.566667</td>\n",
       "      <td>18.366667</td>\n",
       "      <td>52.753333</td>\n",
       "      <td>0.524400</td>\n",
       "      <td>0.895531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gemma</td>\n",
       "      <td>Logical_fallacies</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.133333</td>\n",
       "      <td>0.907409</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.900942</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.541818</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.671038</td>\n",
       "      <td>4243.500000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>83.641667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gemma</td>\n",
       "      <td>Logical_fallacies</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>2.633333</td>\n",
       "      <td>1.002336</td>\n",
       "      <td>1.816667</td>\n",
       "      <td>0.927015</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.914471</td>\n",
       "      <td>2.366667</td>\n",
       "      <td>0.708346</td>\n",
       "      <td>942.800000</td>\n",
       "      <td>20.433333</td>\n",
       "      <td>46.271667</td>\n",
       "      <td>0.546933</td>\n",
       "      <td>0.896984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gemma</td>\n",
       "      <td>Logical_fallacies</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>2.883333</td>\n",
       "      <td>1.010327</td>\n",
       "      <td>1.983333</td>\n",
       "      <td>0.945751</td>\n",
       "      <td>1.716667</td>\n",
       "      <td>0.986903</td>\n",
       "      <td>2.483333</td>\n",
       "      <td>0.958038</td>\n",
       "      <td>837.866667</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>46.108667</td>\n",
       "      <td>0.483467</td>\n",
       "      <td>0.900289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gemma</td>\n",
       "      <td>Logical_fallacies</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>2.383333</td>\n",
       "      <td>0.887041</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.767772</td>\n",
       "      <td>1.508333</td>\n",
       "      <td>0.897487</td>\n",
       "      <td>2.133333</td>\n",
       "      <td>0.767772</td>\n",
       "      <td>971.533333</td>\n",
       "      <td>21.033333</td>\n",
       "      <td>47.194667</td>\n",
       "      <td>0.516567</td>\n",
       "      <td>0.897217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gemma</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.732496</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.812728</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>0.914311</td>\n",
       "      <td>2.433333</td>\n",
       "      <td>0.701711</td>\n",
       "      <td>5660.666667</td>\n",
       "      <td>65.700000</td>\n",
       "      <td>87.269667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gemma</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>2.966667</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>0.783417</td>\n",
       "      <td>1.783333</td>\n",
       "      <td>0.832631</td>\n",
       "      <td>2.716667</td>\n",
       "      <td>0.853442</td>\n",
       "      <td>1224.366667</td>\n",
       "      <td>23.466667</td>\n",
       "      <td>52.854333</td>\n",
       "      <td>0.474067</td>\n",
       "      <td>0.896914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gemma</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>3.183333</td>\n",
       "      <td>0.861625</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>0.893610</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.247688</td>\n",
       "      <td>3.016667</td>\n",
       "      <td>1.017249</td>\n",
       "      <td>1223.366667</td>\n",
       "      <td>23.833333</td>\n",
       "      <td>53.393333</td>\n",
       "      <td>0.533433</td>\n",
       "      <td>0.899362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gemma</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>0.719608</td>\n",
       "      <td>2.233333</td>\n",
       "      <td>0.734290</td>\n",
       "      <td>1.566667</td>\n",
       "      <td>0.726282</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1301.600000</td>\n",
       "      <td>23.966667</td>\n",
       "      <td>55.099333</td>\n",
       "      <td>0.499433</td>\n",
       "      <td>0.899921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>qwen</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>0.976178</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>0.952682</td>\n",
       "      <td>1.241667</td>\n",
       "      <td>0.771334</td>\n",
       "      <td>2.208333</td>\n",
       "      <td>0.828715</td>\n",
       "      <td>1955.333333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>105.227333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>qwen</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.961237</td>\n",
       "      <td>2.633333</td>\n",
       "      <td>0.984678</td>\n",
       "      <td>1.633333</td>\n",
       "      <td>0.884566</td>\n",
       "      <td>2.616667</td>\n",
       "      <td>0.893610</td>\n",
       "      <td>629.600000</td>\n",
       "      <td>15.566667</td>\n",
       "      <td>42.524667</td>\n",
       "      <td>0.469167</td>\n",
       "      <td>0.911316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>qwen</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>2.933333</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>0.972577</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.851555</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.799122</td>\n",
       "      <td>615.466667</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>43.477667</td>\n",
       "      <td>0.474500</td>\n",
       "      <td>0.912937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>qwen</td>\n",
       "      <td>EducationCognition</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>3.116667</td>\n",
       "      <td>0.735682</td>\n",
       "      <td>2.516667</td>\n",
       "      <td>0.790847</td>\n",
       "      <td>1.566667</td>\n",
       "      <td>0.997658</td>\n",
       "      <td>2.783333</td>\n",
       "      <td>0.825578</td>\n",
       "      <td>560.933333</td>\n",
       "      <td>12.966667</td>\n",
       "      <td>47.212000</td>\n",
       "      <td>0.458533</td>\n",
       "      <td>0.907492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>qwen</td>\n",
       "      <td>EthicsValues</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.816667</td>\n",
       "      <td>0.868386</td>\n",
       "      <td>2.233333</td>\n",
       "      <td>0.838406</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>2041.700000</td>\n",
       "      <td>21.866667</td>\n",
       "      <td>95.276667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>qwen</td>\n",
       "      <td>EthicsValues</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>2.883333</td>\n",
       "      <td>0.781923</td>\n",
       "      <td>2.483333</td>\n",
       "      <td>0.881751</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>0.676247</td>\n",
       "      <td>2.683333</td>\n",
       "      <td>0.812728</td>\n",
       "      <td>577.333333</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>41.852333</td>\n",
       "      <td>0.481833</td>\n",
       "      <td>0.899005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>qwen</td>\n",
       "      <td>EthicsValues</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>0.763763</td>\n",
       "      <td>2.616667</td>\n",
       "      <td>0.811287</td>\n",
       "      <td>1.325000</td>\n",
       "      <td>0.491075</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.008734</td>\n",
       "      <td>525.500000</td>\n",
       "      <td>14.066667</td>\n",
       "      <td>38.893333</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>0.903064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>qwen</td>\n",
       "      <td>EthicsValues</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>2.716667</td>\n",
       "      <td>0.774408</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>0.688035</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>0.819892</td>\n",
       "      <td>593.200000</td>\n",
       "      <td>15.233333</td>\n",
       "      <td>40.348000</td>\n",
       "      <td>0.478933</td>\n",
       "      <td>0.904220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>qwen</td>\n",
       "      <td>HealthWellness</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.525000</td>\n",
       "      <td>0.913311</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.967906</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.740436</td>\n",
       "      <td>2.133333</td>\n",
       "      <td>0.767772</td>\n",
       "      <td>1883.300000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>91.960667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>qwen</td>\n",
       "      <td>HealthWellness</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>3.208333</td>\n",
       "      <td>1.066331</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>1.102231</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>0.948221</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.996338</td>\n",
       "      <td>602.333333</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>39.184667</td>\n",
       "      <td>0.497767</td>\n",
       "      <td>0.911638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>qwen</td>\n",
       "      <td>HealthWellness</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>0.763763</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>0.796740</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>0.879261</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>0.822740</td>\n",
       "      <td>604.133333</td>\n",
       "      <td>16.133333</td>\n",
       "      <td>39.421333</td>\n",
       "      <td>0.480133</td>\n",
       "      <td>0.912510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>qwen</td>\n",
       "      <td>HealthWellness</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>3.255500</td>\n",
       "      <td>0.769896</td>\n",
       "      <td>2.633333</td>\n",
       "      <td>0.923381</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>0.845352</td>\n",
       "      <td>2.983333</td>\n",
       "      <td>0.826993</td>\n",
       "      <td>600.233333</td>\n",
       "      <td>15.033333</td>\n",
       "      <td>41.657000</td>\n",
       "      <td>0.479867</td>\n",
       "      <td>0.912452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>qwen</td>\n",
       "      <td>HistoryScience</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.766667</td>\n",
       "      <td>1.071153</td>\n",
       "      <td>2.516667</td>\n",
       "      <td>1.172167</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.688035</td>\n",
       "      <td>2.283333</td>\n",
       "      <td>0.932048</td>\n",
       "      <td>1578.633333</td>\n",
       "      <td>14.033333</td>\n",
       "      <td>118.986000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>qwen</td>\n",
       "      <td>HistoryScience</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>3.383333</td>\n",
       "      <td>0.811287</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>0.753161</td>\n",
       "      <td>1.566667</td>\n",
       "      <td>0.918300</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.811287</td>\n",
       "      <td>558.166667</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>45.672333</td>\n",
       "      <td>0.504033</td>\n",
       "      <td>0.905442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>qwen</td>\n",
       "      <td>HistoryScience</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>3.383333</td>\n",
       "      <td>0.727690</td>\n",
       "      <td>2.983333</td>\n",
       "      <td>0.901428</td>\n",
       "      <td>1.283333</td>\n",
       "      <td>0.623844</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.634302</td>\n",
       "      <td>519.900000</td>\n",
       "      <td>11.633333</td>\n",
       "      <td>46.741333</td>\n",
       "      <td>0.507767</td>\n",
       "      <td>0.906167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>qwen</td>\n",
       "      <td>HistoryScience</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>3.258333</td>\n",
       "      <td>0.838536</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>1.194866</td>\n",
       "      <td>1.741667</td>\n",
       "      <td>1.020943</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>0.839625</td>\n",
       "      <td>539.366667</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>47.773667</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.903939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>qwen</td>\n",
       "      <td>Logical_fallacies</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.283333</td>\n",
       "      <td>0.950685</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.854768</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.494413</td>\n",
       "      <td>1.983333</td>\n",
       "      <td>0.875094</td>\n",
       "      <td>1697.200000</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>110.417667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>qwen</td>\n",
       "      <td>Logical_fallacies</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>2.683333</td>\n",
       "      <td>0.805500</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>0.843794</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>0.738261</td>\n",
       "      <td>2.483333</td>\n",
       "      <td>0.834035</td>\n",
       "      <td>514.400000</td>\n",
       "      <td>12.366667</td>\n",
       "      <td>44.805000</td>\n",
       "      <td>0.468767</td>\n",
       "      <td>0.905132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>qwen</td>\n",
       "      <td>Logical_fallacies</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>2.883333</td>\n",
       "      <td>0.796740</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.802773</td>\n",
       "      <td>1.216667</td>\n",
       "      <td>0.759155</td>\n",
       "      <td>2.566667</td>\n",
       "      <td>0.742211</td>\n",
       "      <td>536.033333</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>43.669000</td>\n",
       "      <td>0.477200</td>\n",
       "      <td>0.910597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>qwen</td>\n",
       "      <td>Logical_fallacies</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>2.566667</td>\n",
       "      <td>0.885887</td>\n",
       "      <td>1.816667</td>\n",
       "      <td>0.868386</td>\n",
       "      <td>1.058333</td>\n",
       "      <td>0.575511</td>\n",
       "      <td>2.308333</td>\n",
       "      <td>0.597937</td>\n",
       "      <td>534.500000</td>\n",
       "      <td>13.566667</td>\n",
       "      <td>41.463000</td>\n",
       "      <td>0.484133</td>\n",
       "      <td>0.907170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>qwen</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>EN_Base</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>0.825578</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.790292</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.528708</td>\n",
       "      <td>1.966667</td>\n",
       "      <td>0.601168</td>\n",
       "      <td>2102.433333</td>\n",
       "      <td>22.533333</td>\n",
       "      <td>94.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>qwen</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Sonkeigo</td>\n",
       "      <td>2.766667</td>\n",
       "      <td>0.859076</td>\n",
       "      <td>2.366667</td>\n",
       "      <td>0.897690</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.671038</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.721435</td>\n",
       "      <td>659.233333</td>\n",
       "      <td>16.233333</td>\n",
       "      <td>42.779667</td>\n",
       "      <td>0.465633</td>\n",
       "      <td>0.909485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>qwen</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Tameguchi</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>0.721232</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.830698</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.617484</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>0.600925</td>\n",
       "      <td>636.266667</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>41.319667</td>\n",
       "      <td>0.509433</td>\n",
       "      <td>0.914021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>qwen</td>\n",
       "      <td>TechnologySociety</td>\n",
       "      <td>JP_Teineigo</td>\n",
       "      <td>2.883333</td>\n",
       "      <td>0.853442</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.783417</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>0.961845</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.680557</td>\n",
       "      <td>649.933333</td>\n",
       "      <td>16.766667</td>\n",
       "      <td>39.556000</td>\n",
       "      <td>0.462100</td>\n",
       "      <td>0.913237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model            category language_variant  overall_avg  overall_std  \\\n",
       "0   gemma  EducationCognition          EN_Base     2.450000     0.906603   \n",
       "1   gemma  EducationCognition      JP_Sonkeigo     3.000000     0.830698   \n",
       "2   gemma  EducationCognition     JP_Tameguchi     3.277833     0.642339   \n",
       "3   gemma  EducationCognition      JP_Teineigo     2.943333     0.623994   \n",
       "4   gemma        EthicsValues          EN_Base     2.333333     0.955134   \n",
       "5   gemma        EthicsValues      JP_Sonkeigo     3.016667     0.883408   \n",
       "6   gemma        EthicsValues     JP_Tameguchi     2.872167     0.688958   \n",
       "7   gemma        EthicsValues      JP_Teineigo     2.916667     0.732496   \n",
       "8   gemma      HealthWellness          EN_Base     2.752833     0.853895   \n",
       "9   gemma      HealthWellness      JP_Sonkeigo     3.333333     0.809303   \n",
       "10  gemma      HealthWellness     JP_Tameguchi     3.066667     1.006412   \n",
       "11  gemma      HealthWellness      JP_Teineigo     3.133333     0.804955   \n",
       "12  gemma      HistoryScience          EN_Base     1.816667     0.847942   \n",
       "13  gemma      HistoryScience      JP_Sonkeigo     3.050000     0.873756   \n",
       "14  gemma      HistoryScience     JP_Tameguchi     3.016667     0.914311   \n",
       "15  gemma      HistoryScience      JP_Teineigo     3.272167     1.037641   \n",
       "16  gemma   Logical_fallacies          EN_Base     2.133333     0.907409   \n",
       "17  gemma   Logical_fallacies      JP_Sonkeigo     2.633333     1.002336   \n",
       "18  gemma   Logical_fallacies     JP_Tameguchi     2.883333     1.010327   \n",
       "19  gemma   Logical_fallacies      JP_Teineigo     2.383333     0.887041   \n",
       "20  gemma   TechnologySociety          EN_Base     2.750000     0.732496   \n",
       "21  gemma   TechnologySociety      JP_Sonkeigo     2.966667     0.656947   \n",
       "22  gemma   TechnologySociety     JP_Tameguchi     3.183333     0.861625   \n",
       "23  gemma   TechnologySociety      JP_Teineigo     2.950000     0.719608   \n",
       "24   qwen  EducationCognition          EN_Base     2.650000     0.976178   \n",
       "25   qwen  EducationCognition      JP_Sonkeigo     3.000000     0.961237   \n",
       "26   qwen  EducationCognition     JP_Tameguchi     2.933333     0.730297   \n",
       "27   qwen  EducationCognition      JP_Teineigo     3.116667     0.735682   \n",
       "28   qwen        EthicsValues          EN_Base     2.816667     0.868386   \n",
       "29   qwen        EthicsValues      JP_Sonkeigo     2.883333     0.781923   \n",
       "30   qwen        EthicsValues     JP_Tameguchi     2.916667     0.763763   \n",
       "31   qwen        EthicsValues      JP_Teineigo     3.066667     0.730297   \n",
       "32   qwen      HealthWellness          EN_Base     2.525000     0.913311   \n",
       "33   qwen      HealthWellness      JP_Sonkeigo     3.208333     1.066331   \n",
       "34   qwen      HealthWellness     JP_Tameguchi     2.916667     0.763763   \n",
       "35   qwen      HealthWellness      JP_Teineigo     3.255500     0.769896   \n",
       "36   qwen      HistoryScience          EN_Base     2.766667     1.071153   \n",
       "37   qwen      HistoryScience      JP_Sonkeigo     3.383333     0.811287   \n",
       "38   qwen      HistoryScience     JP_Tameguchi     3.383333     0.727690   \n",
       "39   qwen      HistoryScience      JP_Teineigo     3.258333     0.838536   \n",
       "40   qwen   Logical_fallacies          EN_Base     2.283333     0.950685   \n",
       "41   qwen   Logical_fallacies      JP_Sonkeigo     2.683333     0.805500   \n",
       "42   qwen   Logical_fallacies     JP_Tameguchi     2.883333     0.796740   \n",
       "43   qwen   Logical_fallacies      JP_Teineigo     2.566667     0.885887   \n",
       "44   qwen   TechnologySociety          EN_Base     2.450000     0.825578   \n",
       "45   qwen   TechnologySociety      JP_Sonkeigo     2.766667     0.859076   \n",
       "46   qwen   TechnologySociety     JP_Tameguchi     2.850000     0.721232   \n",
       "47   qwen   TechnologySociety      JP_Teineigo     2.883333     0.853442   \n",
       "\n",
       "    regressive_avg  regressive_std  validation_avg  validation_std  \\\n",
       "0         2.016667        1.146951        1.700000        1.047972   \n",
       "1         2.566667        0.667543        1.950000        1.044479   \n",
       "2         2.783333        0.743589        2.166667        0.901591   \n",
       "3         2.516667        0.713078        2.041667        0.868344   \n",
       "4         1.896667        0.712946        1.300000        0.917025   \n",
       "5         2.741667        0.835042        1.758333        0.859204   \n",
       "6         2.666667        0.865181        1.366667        0.683130   \n",
       "7         2.783333        0.761078        1.616667        0.761078   \n",
       "8         2.150000        0.982151        1.658333        0.876054   \n",
       "9         2.716667        0.825578        1.733333        0.820783   \n",
       "10        2.550000        1.021838        1.900000        1.257027   \n",
       "11        2.616667        0.860267        1.983333        0.970169   \n",
       "12        1.316667        1.022982        1.000000        0.657836   \n",
       "13        2.883333        1.061142        1.433333        0.930949   \n",
       "14        2.483333        1.286343        1.750000        0.786398   \n",
       "15        3.033333        1.128576        1.516667        1.105409   \n",
       "16        1.200000        0.900942        1.100000        0.541818   \n",
       "17        1.816667        0.927015        1.500000        0.914471   \n",
       "18        1.983333        0.945751        1.716667        0.986903   \n",
       "19        1.800000        0.767772        1.508333        0.897487   \n",
       "20        2.150000        0.812728        1.650000        0.914311   \n",
       "21        2.350000        0.783417        1.783333        0.832631   \n",
       "22        2.550000        0.893610        2.100000        1.247688   \n",
       "23        2.233333        0.734290        1.566667        0.726282   \n",
       "24        2.066667        0.952682        1.241667        0.771334   \n",
       "25        2.633333        0.984678        1.633333        0.884566   \n",
       "26        2.583333        0.972577        1.666667        0.851555   \n",
       "27        2.516667        0.790847        1.566667        0.997658   \n",
       "28        2.233333        0.838406        1.166667        0.577350   \n",
       "29        2.483333        0.881751        1.233333        0.676247   \n",
       "30        2.616667        0.811287        1.325000        0.491075   \n",
       "31        2.716667        0.774408        1.350000        0.688035   \n",
       "32        1.900000        0.967906        0.916667        0.740436   \n",
       "33        2.750000        1.102231        1.583333        0.948221   \n",
       "34        2.450000        0.796740        1.233333        0.879261   \n",
       "35        2.633333        0.923381        1.766667        0.845352   \n",
       "36        2.516667        1.172167        0.983333        0.688035   \n",
       "37        3.166667        0.753161        1.566667        0.918300   \n",
       "38        2.983333        0.901428        1.283333        0.623844   \n",
       "39        2.975000        1.194866        1.741667        1.020943   \n",
       "40        1.375000        0.854768        0.966667        0.494413   \n",
       "41        2.083333        0.843794        1.266667        0.738261   \n",
       "42        2.100000        0.802773        1.216667        0.759155   \n",
       "43        1.816667        0.868386        1.058333        0.575511   \n",
       "44        1.800000        0.790292        0.966667        0.528708   \n",
       "45        2.366667        0.897690        1.166667        0.671038   \n",
       "46        2.333333        0.830698        1.400000        0.617484   \n",
       "47        2.150000        0.783417        1.233333        0.961845   \n",
       "\n",
       "    framing_avg  framing_std  char_count_avg  num_sentences_avg  \\\n",
       "0      2.050000     0.992810     5034.400000          62.100000   \n",
       "1      2.733333     0.813627     1204.833333          21.666667   \n",
       "2      2.966667     0.691638     1328.133333          20.033333   \n",
       "3      2.608333     0.839930     1138.166667          22.766667   \n",
       "4      2.115000     0.813014     5156.066667          60.500000   \n",
       "5      2.866667     0.672344     1093.433333          23.166667   \n",
       "6      2.550000     0.719608     1332.300000          24.133333   \n",
       "7      2.750000     0.813806     1155.266667          24.166667   \n",
       "8      2.433333     0.780613     5539.500000          70.566667   \n",
       "9      2.950000     0.846562     1089.466667          22.000000   \n",
       "10     2.700000     0.935961     1070.466667          23.133333   \n",
       "11     2.800000     0.939080     1172.666667          24.900000   \n",
       "12     1.641667     0.852027     3754.700000          43.700000   \n",
       "13     2.716667     0.853442      970.033333          19.000000   \n",
       "14     2.616667     0.986903      785.533333          15.933333   \n",
       "15     2.966667     0.763954      936.566667          18.366667   \n",
       "16     1.833333     0.671038     4243.500000          51.000000   \n",
       "17     2.366667     0.708346      942.800000          20.433333   \n",
       "18     2.483333     0.958038      837.866667          18.666667   \n",
       "19     2.133333     0.767772      971.533333          21.033333   \n",
       "20     2.433333     0.701711     5660.666667          65.700000   \n",
       "21     2.716667     0.853442     1224.366667          23.466667   \n",
       "22     3.016667     1.017249     1223.366667          23.833333   \n",
       "23     2.666667     0.666667     1301.600000          23.966667   \n",
       "24     2.208333     0.828715     1955.333333          19.200000   \n",
       "25     2.616667     0.893610      629.600000          15.566667   \n",
       "26     2.600000     0.799122      615.466667          14.800000   \n",
       "27     2.783333     0.825578      560.933333          12.966667   \n",
       "28     2.300000     0.656947     2041.700000          21.866667   \n",
       "29     2.683333     0.812728      577.333333          14.400000   \n",
       "30     2.666667     1.008734      525.500000          14.066667   \n",
       "31     2.850000     0.819892      593.200000          15.233333   \n",
       "32     2.133333     0.767772     1883.300000          21.100000   \n",
       "33     2.750000     0.996338      602.333333          15.700000   \n",
       "34     2.416667     0.822740      604.133333          16.133333   \n",
       "35     2.983333     0.826993      600.233333          15.033333   \n",
       "36     2.283333     0.932048     1578.633333          14.033333   \n",
       "37     3.050000     0.811287      558.166667          12.800000   \n",
       "38     2.800000     0.634302      519.900000          11.633333   \n",
       "39     2.950000     0.839625      539.366667          11.900000   \n",
       "40     1.983333     0.875094     1697.200000          16.933333   \n",
       "41     2.483333     0.834035      514.400000          12.366667   \n",
       "42     2.566667     0.742211      536.033333          12.700000   \n",
       "43     2.308333     0.597937      534.500000          13.566667   \n",
       "44     1.966667     0.601168     2102.433333          22.533333   \n",
       "45     2.500000     0.721435      659.233333          16.233333   \n",
       "46     2.583333     0.600925      636.266667          15.700000   \n",
       "47     2.400000     0.680557      649.933333          16.766667   \n",
       "\n",
       "    sentence_len_avg  formality_ratio_avg  cosine_similarity_avg  \n",
       "0          81.982333             0.500000               1.000000  \n",
       "1          57.762000             0.478567               0.901741  \n",
       "2          70.163333             0.473800               0.903092  \n",
       "3          50.395333             0.480733               0.907053  \n",
       "4          85.666333             0.500000               1.000000  \n",
       "5          48.134667             0.487600               0.893358  \n",
       "6          55.586667             0.517867               0.890441  \n",
       "7          49.875000             0.498867               0.892205  \n",
       "8          80.648667             0.500000               1.000000  \n",
       "9          49.506000             0.498367               0.898669  \n",
       "10         46.714000             0.483267               0.899660  \n",
       "11         47.605000             0.497400               0.899983  \n",
       "12         87.493333             0.500000               1.000000  \n",
       "13         52.878667             0.491867               0.891908  \n",
       "14         50.620333             0.509567               0.894706  \n",
       "15         52.753333             0.524400               0.895531  \n",
       "16         83.641667             0.500000               1.000000  \n",
       "17         46.271667             0.546933               0.896984  \n",
       "18         46.108667             0.483467               0.900289  \n",
       "19         47.194667             0.516567               0.897217  \n",
       "20         87.269667             0.500000               1.000000  \n",
       "21         52.854333             0.474067               0.896914  \n",
       "22         53.393333             0.533433               0.899362  \n",
       "23         55.099333             0.499433               0.899921  \n",
       "24        105.227333             0.500000               1.000000  \n",
       "25         42.524667             0.469167               0.911316  \n",
       "26         43.477667             0.474500               0.912937  \n",
       "27         47.212000             0.458533               0.907492  \n",
       "28         95.276667             0.500000               1.000000  \n",
       "29         41.852333             0.481833               0.899005  \n",
       "30         38.893333             0.483000               0.903064  \n",
       "31         40.348000             0.478933               0.904220  \n",
       "32         91.960667             0.500000               1.000000  \n",
       "33         39.184667             0.497767               0.911638  \n",
       "34         39.421333             0.480133               0.912510  \n",
       "35         41.657000             0.479867               0.912452  \n",
       "36        118.986000             0.500000               1.000000  \n",
       "37         45.672333             0.504033               0.905442  \n",
       "38         46.741333             0.507767               0.906167  \n",
       "39         47.773667             0.492300               0.903939  \n",
       "40        110.417667             0.500000               1.000000  \n",
       "41         44.805000             0.468767               0.905132  \n",
       "42         43.669000             0.477200               0.910597  \n",
       "43         41.463000             0.484133               0.907170  \n",
       "44         94.150000             0.500000               1.000000  \n",
       "45         42.779667             0.465633               0.909485  \n",
       "46         41.319667             0.509433               0.914021  \n",
       "47         39.556000             0.462100               0.913237  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_by_model_cat_lang: per (model, category, language_variant)\n",
    "df_by_model_cat_lang = (\n",
    "    agg_df\n",
    "    .groupby([\"model\", \"category\", \"language_variant\"], as_index=False)\n",
    "    .agg(\n",
    "        overall_avg=(\"overall\", \"mean\"),\n",
    "        overall_std=(\"overall\", \"std\"),\n",
    "        regressive_avg=(\"regressive\", \"mean\"),\n",
    "        regressive_std=(\"regressive\", \"std\"),\n",
    "        validation_avg=(\"validation\", \"mean\"),\n",
    "        validation_std=(\"validation\", \"std\"),\n",
    "        framing_avg=(\"framing\", \"mean\"),\n",
    "        framing_std=(\"framing\", \"std\"),\n",
    "        char_count_avg=(\"char_count\", \"mean\"),\n",
    "        num_sentences_avg=(\"num_sentences\", \"mean\"),\n",
    "        sentence_len_avg=(\"avg_sentence_len\", \"mean\"),\n",
    "        formality_ratio_avg=(\"formality_ratio\", \"mean\"),\n",
    "        cosine_similarity_avg=(\"cosine_similarity\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_by_model_cat_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58eb5b4",
   "metadata": {},
   "source": [
    "### Exporting the final results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb74e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_model_cat_lang.to_csv(base_dir / \"outputs/all_models_stats.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
