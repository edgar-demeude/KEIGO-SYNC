%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for ECAI Papers 
%%% Prepared by Ulle Endriss (version 1.0 of 2023-12-10)

%%% To be used with the ECAI class file ecai.cls.
%%% You also will need a bibliography file (such as mybibfile.bib).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass{} command.
%%% Use the first variant for the camera-ready paper.
%%% Use the second variant for submission (for double-blind reviewing).

\documentclass{ecai} 
%\documentclass[doubleblind]{ecai} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Load any packages you require here. 

\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{color}
\usepackage{tabularx}
\usepackage{float}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any theorem-like environments you require here.

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any new commands you require here.

\newcommand{\BibTeX}{B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frontmatter}

%%% Use this command to specify your submission number.
%%% In doubleblind mode, it will be printed on the first page.

\paperid{123} 

%%% Use this command to specify the title of your paper.

\title{KEIGO-SYNC : Evaluating the Impact of Politeness Levels
on Factual Reliability in Japanese Large Language Models
}

%%% Use this combinations of commands to specify all authors of your 
%%% paper. Use \fnms{} and \snm{} to indicate everyone's first names 
%%% and surname. This will help the publisher with indexing the 
%%% proceedings. Please use a reasonable approximation in case your 
%%% name does not neatly split into "first names" and "surname".
%%% Specifying your ORCID digital identifier is optional. 
%%% Use the \thanks{} command to indicate one or more corresponding 
%%% authors and their email address(es). If so desired, you can specify
%%% author contributions using the \footnote{} command.

\author[]{\fnms{Aubourg}~\snm{Thomas}}
\author[]{\fnms{Demeude}~\snm{Edgar}}
\author[]{\fnms{Dupuy}~\snm{Guilhem}}
\author[]{\fnms{Vu}~\snm{Anh Duy}}
\end{frontmatter}
%%% Use this environment to include an abstract of your paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Sycophancy in large language models (LLMs)---the tendency to agree with user-stated beliefs 
conflicting with verifiable facts---constitutes a persistent alignment challenge. 
While existing research examines sycophantic behavior primarily in English, the role of 
grammatical register as a systematic factor remains insufficiently explored, particularly in 
high-context languages where politeness is grammatically encoded.
\\
KEIGO-SYNC introduces a controlled evaluation framework measuring the relationship between grammatical 
politeness and sycophantic agreement using Japanese, where honorific speech (Keigo) encodes social deference through 
obligatory syntactic structures. Each prompt is instantiated across four register conditions: English Neutral, 
Japanese Tameguchi (casual), Teineigo (polite), and Sonkeigo (honorific), enabling empirical assessment of 
whether linguistic deference correlates with measurable changes in model agreement behavior.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dataset Construction}

The corpus comprises 60 prompts designed to isolate the effect of grammatical politeness on factual integrity. 
To ensure diversity and comparability, the dataset integrates items from the Logical Fallacy Detection\cite{jin2022logicalfallacydetection} benchmark, 
the ELEPHANT dataset, and original questions specifically developed for this study. These items are organized into 
five thematic domains: education, health, logical fallacies, technology, and ethics, with topics restricted to 
widely documented general knowledge to limit specialized knowledge biases.
\\
All items follow a unified [Assertion] + [Question] template, formulated assertively and deliberately incorporating
 erroneous premises or reasoning biases to measure models' ability to maintain critical stance under linguistic
  pressure. Originally written in English, all prompts were adapted by a native Japanese speaker to ensure semantic 
  and pragmatic fidelity. Each item was instantiated across three Japanese registers: 
  Tameguchi (informal), Teineigo (standard), and Sonkeigo (honorific). By treating politeness level as the sole 
  independent variable, this design enables controlled analysis of how linguistic deference influences the rate 
  of sycophantic agreement versus factual correction.
  
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Setup}

\subsection{Model Selection and Configuration}


To establish a baseline for evaluating sycophantic behavior across diverse linguistic frameworks, we selected a representative suite of Large Language Models (LLMs) characterized by varying architectural designs and regional development origins. 
The selection process was governed by three primary independent variables: geographic-linguistic origin, parameter scale, and the specific alignment methodologies employed during the training of the response-generation models.
\\
To examine the influence of cultural and linguistic priors embedded in training corpora, the study contrasts models of Asian origin, specifically Qwen 2.5 (7B) and GLM-4.6V-Flash, with those of Western origin, namely Gemma 3 (27B) and Mistral (7B).
 This geographic stratification facilitates an analysis of whether models developed within high-context linguistic environments demonstrate distinct sycophantic patterns compared to those trained predominantly on Western data when subjected to complex Japanese honorific constraints.
\\
The experimental design further accounts for structural variability by incorporating a spectrum of model capacities ranging from 7B to 27B parameters.
 This gradient enables an investigation into the correlation between parameter scale and a model’s capacity to maintain factual consistency under social or grammatical pressure. 
 A critical component of this protocol is the inclusion of the Mistral 7B base model, which serves as a non-aligned baseline as it has not undergone Reinforcement Learning from Human Feedback (RLHF).
 The use of a non-RLHF model allows for the isolation of sycophancy as a potential emergent property of alignment processes designed to prioritize user satisfaction over factual adherence. 
 To ensure statistical rigor and reproducibility, all inferences were conducted with a temperature setting of 0.5, and each prompt was subjected to N=3 independent iterations to mitigate the impact of stochastic variance.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{LLM-as-Judge Framework}

Factual correctness and sycophantic behavior are evaluated using an LLM-as-judge framework with dedicated bilingual English–Japanese judging models. The primary evaluation is conducted using Qwen 2.5-7B Instruct and Meta’s Llama 3.2 3B-Instruct. To ensure validity and alignment, these judgments are complemented by Google’s Gemma 3 (27B) on a 15\% subsample and human verification on a 7.5\% subsample. All judges are fully isolated from the answer-generation models and have no access to the trap construction process, prompt variants, or model identities, ensuring evaluations rely solely on the observable content of the responses.
\\
Each evaluation instance provides the judge with an English factual reference answer, the model response, and structured scoring instructions.
 The English reference anchors the evaluation to a consistent factual baseline and reduces cross-lingual ambiguity. 
 Judges assess semantic content and factual alignment, ignoring surface-level stylistic or politeness differences unless they interfere with factual accuracy.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluation Metrics}

\subsubsection{LLM-as-Judge}
Sycophancy is assessed using a structured evaluation prompt that instructs the judge to quantify the extent to which a model response prioritizes agreement, flattery, or face-preserving behavior over objective and factually accurate content. 
The prompt operationalizes sycophancy along three dimensions derived from the ELEPHANT framework\cite{cheng2025elephant}: regressive sycophancy, validation sycophancy, and framing sycophancy.
\textbf{Regressive sycophancy} captures the degree to which the model sacrifices factual accuracy or independent reasoning to conform to the user’s incorrect beliefs. 
\textbf{Validation sycophancy} measures the extent to which the response emphasizes emotional reassurance or praise over objective guidance. 
\textbf{Framing sycophancy} evaluates whether the model uncritically accepts the user’s assumptions or framing rather than examining or correcting them when necessary. 
Each dimension is scored on a five-point scale, with higher values corresponding to greater prioritization of user appeasement over truthfulness. 
While the prompt asks judges to provide a holistic overall rating, the final sycophancy score used in our analysis is calculated as the arithmetic mean of these three dimensional scores, ensuring a balanced representation of the behavior.
\\
The prompt includes explicit instructions to compare the model response against a provided English reference answer, treated as the factual ground truth. 
Judges are guided to distinguish between responses that accurately correct or challenge misleading statements, partially align with the ground truth, or fully adopt erroneous premises. 
Brief politeness or minor empathetic language is explicitly excluded from high sycophancy ratings, whereas repeated validation, excessive agreement, or uncritical adoption of misleading premises is penalized. 
To ensure data consistency and valid formatting, we employ a separate instance of Qwen Instruct to parse the raw output from the judges. This parsing step extracts the three dimension scores, the computed average, and the concise textual rationale (two to four sentences) into a clean, standardized JSON format.
This structured workflow ensures that evaluations are reproducible, interpretable, and aligned with a clearly defined operationalization of sycophancy.
        
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Quantitative Metrics}
To quantify the trade-off between social deference and factual accuracy, the system employs a custom metric engine that computes two primary independent variables. 
The first, the Formality Ratio, measures the degree of politeness in the model’s response. 
It is normalized between 0.0 and 1.0 and calculated using rule-based detection of specific Japanese honorific markers. 
The scoring assigns the highest weight to Sonkeigo verbs (e.g., meshiagaru, zonjimasu, 1.0), moderate weight to standard Teineigo copulas (desu/masu, 0.6), and low weight to casual Tameguchi particles (daro, jan, 0.2). 
The second variable, Factual Adherence, quantifies the semantic alignment of the response with the factual baseline. 
This is computed by embedding the Japanese model response using Google’s Gemini Embedding 001 model and calculating the cosine similarity against the embedding of the corresponding English ground truth.
\\
By correlating the Formality Ratio with Factual Adherence, the framework identifies instances of sycophantic drift, defined as cases in which the model sacrifices factual accuracy to comply with the grammatical constraints of high-context politeness. 
This approach enables a precise, quantitative assessment of how increasing levels of linguistic deference influence the model’s ability to maintain truthfulness.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation Protocol}

\subsection{Human Evaluation of LLMs as a Judge}

To validate the efficacy of the LLM-as-a-judge framework, the automated metrics were benchmarked against human judgment using a representative subsample of the dataset ($n=72$~responses, derived from 18 of the 60 primary questions across four distinct models). 
This subset accounts for 7.5\% of the total response corpus analyzed in this study. 
To mitigate potential systemic bias, the human evaluation was conducted by native Japanese speakers external to the research project, ensuring an objective baseline.
\\
The protocol prioritizes the alignment of evaluative trends over absolute score magnitude. 
Specifically, the framework assesses the inter-rater reliability between human judges and individual LLM judges through two statistical lenses: Spearman’s rank correlation coefficient, to evaluate the consistency of model rankings by quality, and Pearson’s correlation coefficient, to quantify the linear relationship and magnitude alignment between the scores.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Statistical Analysis and Robustness Checks}

The following table presents the statistical alignment between human evaluators and the three LLM judges regarding the \textit{framing} metric.
\begin{table}[H]
    \centering
    \small 
    \caption{Inter-rater Reliability: Spearman and Pearson Correlations for Framing}
    \label{tab:framing_correlation}
    \begin{tabularx}{\columnwidth}{l @{\extracolsep{\fill}} ccc}
        \toprule
        & \multicolumn{3}{c}{Judge model} \\
        \cmidrule(lr){2-4}
        Metric & \scriptsize Gemma 3-27B & \scriptsize Llama 3.2-3B & \scriptsize Qwen 2.5-7B \\
        \midrule
        \textbf{Pearson Linear ($r$)} & & & \\
        \quad Coeff. & \textbf{0.4539} & 0.3413 & 0.1790 \\
        \quad $p$-value & (0.0006) & (0.0115) & (0.1952) \\
        \addlinespace
        \textbf{Spearman Rank ($\rho$)} & & & \\
        \quad Coeff. & \textbf{0.4075} & 0.2986 & 0.1650 \\
        \quad $p$-value & (0.0022) & (0.0283) & (0.2332) \\
        \bottomrule
    \end{tabularx}
\end{table}

The empirical data indicates a lack of statistically significant correlation for the \textbf{Qwen 2.5-7B Instruct} model ($p > 0.19$ for both coefficients). 
Consequently, this model was excluded from subsequent analysis as its evaluative output does not reliably reflect human judgment. In contrast, both \textbf{Gemma 3-27B} and \textbf{Llama 3.2-3B Instruct} demonstrate a positive correlation with human ground truth.
While the correlation coefficients remain moderate ($r \approx 0.30\text{--}0.45$), the remarkably low $p$-values ($p < 0.05$) confirm that these associations are statistically robust and not the result of stochastic variation.
\\
The degree of alignment with human evaluators also appears to scale positively with model parameters and general reasoning capabilities.
\\
Furthermore, a significant divergence in absolute score magnitude was observed. Human evaluators assigned a mean sycophancy score of 0.80, whereas \textbf{Gemma 3-27B} yielded a mean of 2.09. 
This discrepancy suggests a fundamental difference in evaluative calibration: while the trends remain correlated, LLMs exhibit a higher sensitivity to sycophantic markers, resulting in more ``stringent'' or ``harsh'' grading behaviors compared to human subjects.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Results}

The empirical evaluation reveals a consistent but nuanced relationship between grammatical politeness and sycophantic behavior across all evaluated judges. Table 1 presents average sycophancy scores across four language variants, assessed by human evaluators and two LLM-based judges.

\begin{table}[H]
    \centering
    \small 
    \caption{Average sycophancy scores by judge and language variant}
    \label{tab:sycophancy_scores}
    \begin{tabularx}{\columnwidth}{lXXX}
        \toprule
        & \multicolumn{3}{c}{Judge model} \\
        \cmidrule(lr){2-4}
        \scriptsize Language variant & \scriptsize Human & \scriptsize Gemma \mbox{3-27B} & \scriptsize Llama 3.2-3B Instruct \\
        \midrule
            EN\_Base & 0.62 & 1.39 & 1.87 \\
            JP\_Tameguchi & 0.88 & 1.82 & 2.11 \\
            JP\_Teineigo & 0.79 & 1.79 & 2.23 \\
            JP\_Sonkeigo & \textbf{0.90} & \textbf{1.98} & \textbf{2.28} \\
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Dominance of Language and Honorifics}

The transition from English to Japanese emerged as the strongest predictor of sycophantic behavior. 
The mean absolute difference between the English baseline and Japanese variants ($\Delta_{\text{EN}\rightarrow\text{JP}} = 0.47$ for Gemma) was over twice the magnitude of internal register variations, indicating that the language shift itself drives the bulk of the behavioral change.

Crucially, however, the hyper-formal **Sonkeigo** register consistently yielded the highest sycophancy scores across all three evaluation conditions, validating the hypothesized politeness-truthfulness trade-off. 
Gemma 3-27B recorded a 42.4\% increase from the English baseline to Sonkeigo, while Llama 3.2-3B showed a 21.9\% increase. 
Most notably, human evaluators demonstrated a 45.2\% spike, confirming that excessive grammatical deference correlates strongly with increased agreement.

\subsection{Non-Linearity and Calibration}

While the extremes (English vs. Sonkeigo) followed a predictable pattern, the intermediate **Teineigo** register defied monotonic expectations. 
Both Human and Gemma 3 evaluators rated standard politeness (Teineigo) lower than casual speech (Tameguchi), with human scores dropping from 0.88 to 0.79. 
This suggests that while extreme formality drives sycophancy, standard politeness may trigger different behavioral mechanisms.
Regarding calibration, although human evaluators applied stricter standards (mean = 0.80) than LLM judges (Gemma: 1.75, Llama: 2.12), all judges preserved the fundamental ordering of English $<$ Japanese, confirming the robustness of these findings despite baseline sensitivity differences.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

This study introduces KEIGO-SYNC, a controlled evaluation framework for measuring the relationship between grammatical politeness and sycophantic behavior in multilingual large language models. 
By isolating honorific register as an independent variable while maintaining semantic invariance, we provide empirical evidence for a language-level sycophancy gap that substantially exceeds intra-register variation within Japanese.

\subsection{Principal Findings}
Our results demonstrate two primary phenomena. 
First, a consistent cross-linguistic effect emerges whereby Japanese prompts elicit systematically higher sycophancy scores than semantically equivalent English prompts, with language transition effects ($\Delta_{\text{EN}\rightarrow\text{JP}}$) approximately 2--3 times larger than maximum intra-Japanese register differences. 
Second, while Sonkeigo (honorific) register exhibited the highest sycophancy scores in three of four judge conditions, the gradation across Japanese registers (Tameguchi, Teineigo, Sonkeigo) displayed non-monotonic patterns suggesting complex interactions between grammatical formality, cultural priors embedded in training data, and model-specific alignment objectives.

\subsection{Limitations and Methodological Constraints}

Several methodological limitations constrain the generalizability of these findings and warrant explicit acknowledgment.


\textbf{Model Constraints.} The LLM-as-judge evaluation relied on models of limited capacity (Gemma 3-27B, Llama 3.2-3B) due to budgetary and computational constraints.
 Ideally, frontier models such as Gemini 3 or GPT-5 would serve as judges to maximize linguistic sophistication and cross-cultural reasoning capability.
 The use of smaller open-source models, while enabling reproducibility, may have introduced evaluation noise or failed to capture subtle semantic nuances in Japanese honorific constructions.


\textbf{Dataset Scale.} The evaluation corpus comprised only 60 prompts, falling substantially short of the initially projected target of 200+ items.
 This limited sample size constrains statistical power for detecting interaction effects between register, semantic domain, and model architecture.
 Fine-grained analyses of domain-specific or fallacy-type-specific sycophancy patterns remain underpowered, and confidence intervals around effect size estimates are correspondingly wide.


\textbf{Prompt Design Uniformity.} All experimental items followed a rigid [Assertion] + [Question] template structure.
 This uniformity, while controlling for structural confounds, limits ecological validity.
 Real-world sycophancy traps exhibit substantial variation in rhetorical framing, interrogative vs. declarative phrasing, and implicit vs. explicit false premises.
 The absence of paraphrastic variation and alternative formulation strategies may have artificially stabilized model responses, potentially underestimating the true variance in sycophantic behavior across naturalistic interaction contexts.


\textbf{Translation Quality Assurance.} While all Japanese translations were produced by a native speaker, the absence of formal linguistic expertise in Japanese honorific pragmatics introduces potential validity threats.
 Subtle errors in Keigo application, inappropriate register mixing, or unnatural formality gradations may have inadvertently confounded the manipulation.
 Ideally, translations would undergo expert review by a specialist in Japanese sociolinguistics or undergo inter-rater reliability assessment by multiple native speakers with metalinguistic training.


\subsection{Future Directions}

Despite these limitations, KEIGO-SYNC establishes a replicable methodology for isolating grammatical register effects in safety-critical model behaviors.
 Future work should expand the dataset to include additional high-context languages (e.g., Korean, Thai, Javanese), increase sample sizes to enable robust statistical inference, diversify prompt structures to capture naturalistic variation, and employ frontier judge models or specialized bilingual annotators.
 Additionally, controlled ablation studies examining the interaction between RLHF intensity and honorific-induced sycophancy could clarify whether this phenomenon is intrinsic to alignment procedures or correctable through modified training objectives.
 Ultimately, addressing the politeness-truthfulness trade-off represents a necessary condition for the safe deployment of culturally-aware language models in global settings.

\section{Citations and references}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to include your bibliography file.

\bibliography{references}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
