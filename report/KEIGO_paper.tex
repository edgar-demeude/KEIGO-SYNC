%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for ECAI Papers 
%%% Prepared by Ulle Endriss (version 1.0 of 2023-12-10)

%%% To be used with the ECAI class file ecai.cls.
%%% You also will need a bibliography file 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass{} command.
%%% Use the first variant for the camera-ready paper.
%%% Use the second variant for submission (for double-blind reviewing).

\documentclass{ecai} 
%\documentclass[doubleblind]{ecai} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Load any packages you require here. 

\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{color}
\usepackage{tabularx}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
}
\usepackage{caption}
% Sets a margin of 10pt on the left and right for all captions
\captionsetup{margin=10pt, font=small, labelfont=bf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any theorem-like environments you require here.

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any new commands you require here.

\newcommand{\BibTeX}{B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frontmatter}

%%% Use this command to specify your submission number.
%%% In doubleblind mode, it will be printed on the first page.

\paperid{123} 

%%% Use this command to specify the title of your paper.

\title{KEIGO-SYNC : Evaluating the Impact of Politeness Levels
on Factual Reliability in Japanese Large Language Models
}

%%% Use this combinations of commands to specify all authors of your 
%%% paper. Use \fnms{} and \snm{} to indicate everyone's first names 
%%% and surname. This will help the publisher with indexing the 
%%% proceedings. Please use a reasonable approximation in case your 
%%% name does not neatly split into "first names" and "surname".
%%% Specifying your ORCID digital identifier is optional. 
%%% Use the \thanks{} command to indicate one or more corresponding 
%%% authors and their email address(es). If so desired, you can specify
%%% author contributions using the \footnote{} command.

\author[]{\fnms{Aubourg}~\snm{Thomas}}
\author[]{\fnms{Demeude}~\snm{Edgar}}
\author[]{\fnms{Dupuy}~\snm{Guilhem}}
\author[]{\fnms{Vu}~\snm{Anh Duy}}
\address{\vspace{0.5em} \centering \small
    \textit{Code : \url{https://github.com/edgar-demeude/KEIGO-SYNC}} \ | \
    \textit{Project Video Recap: \url{https://www.youtube.com/watch?v=9FBMZSCvf-g}}
}

\end{frontmatter}
%%% Use this environment to include an abstract of your paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Sycophancy in large language models (LLMs)---the tendency to agree with user-stated beliefs 
conflicting with verifiable facts---constitutes a persistent alignment challenge \cite{sharma2023towards}. 
While existing research examines sycophantic behavior primarily in English \cite{armengol2024quantifying}, the role of 
grammatical register as a systematic factor remains insufficiently explored, particularly in 
high-context languages where politeness is grammatically encoded \cite{liang2022holistic}.
\\
KEIGO-SYNC introduces a controlled evaluation framework measuring the relationship between grammatical 
politeness and sycophantic agreement using Japanese, where honorific speech (Keigo) encodes social deference through 
obligatory syntactic structures. Each prompt is instantiated across four register conditions: English Neutral, 
Japanese Tameguchi (casual), Teineigo (polite), and Sonkeigo (honorific), enabling empirical assessment of 
whether linguistic deference correlates with measurable changes in model agreement behavior.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dataset Construction}

The corpus comprises 60 prompts designed to isolate the effect of grammatical politeness on factual integrity. 
To ensure diversity and comparability, the dataset integrates items from the Logical Fallacy Detection\cite{jin2022logicalfallacydetection} benchmark, 
the ELEPHANT dataset, and original questions specifically developed for this study. These items are organized into 
five thematic domains: education, health, logical fallacies, technology, and ethics, drawing on thematic categories established in benchmarks like MMLU \cite{hendrycks2020measuring} and the Logical Fallacy Detection dataset \cite{jin2022logicalfallacydetection}
Topics are  restricted to widely documented general knowledge to limit specialized knowledge biases.
\\
All items follow a unified [Assertion] + [Question] template, formulated assertively and deliberately incorporating
erroneous premises or reasoning biases to measure models' ability to maintain critical stance under linguistic
pressure. Originally written in English, all prompts were adapted by a native Japanese speaker to ensure semantic 
and pragmatic fidelity. Each item was instantiated across three Japanese registers: 
Tameguchi (informal), Teineigo (standard), and Sonkeigo (honorific). By treating politeness level as the sole 
independent variable, this design enables controlled analysis of how linguistic deference influences the rate 
of sycophantic agreement versus factual correction.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Setup}

\subsection{Model Selection and Configuration}


We evaluated models of varying origins and scales: Asian-origin (Qwen 2.5 7B, GLM-4.6V-Flash) versus Western-origin (Gemma 3 27B, Mistral v0.3 7B), spanning 7B to 27B parameters. Mistral 7B base served as a non-RLHF baseline. All inferences used temperature 0.5 with N=3 iterations.
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{LLM-as-Judge Framework}

Factual correctness and sycophantic behavior are evaluated using an LLM-as-judge framework with dedicated bilingual English–Japanese judging models. The primary evaluation is conducted using Qwen 2.5-7B Instruct and Meta’s Llama 3.2 3B-Instruct. To ensure validity and alignment, these judgments are complemented by Google’s Gemma 3 (27B) on a 15\% subsample and human verification on a 10\% subsample. All judges are fully isolated from the answer-generation models and have no access to the trap construction process, prompt variants, or model identities, ensuring evaluations rely solely on the observable content of the responses.
\\
Each evaluation instance provides the judge with an English factual reference answer, the model response, and structured scoring instructions.
 The English reference anchors the evaluation to a consistent factual baseline and reduces cross-lingual ambiguity. 
 Judges assess semantic content and factual alignment, ignoring surface-level stylistic or politeness differences unless they interfere with factual accuracy.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluation Metrics}

\subsubsection{LLM-as-Judge}
Sycophancy is assessed using a structured evaluation prompt operationalizing three dimensions derived from the ELEPHANT framework\cite{cheng2025elephant}: \textbf{regressive sycophancy} (sacrificing factual accuracy to conform to user's incorrect beliefs), \textbf{validation sycophancy} (emphasizing emotional reassurance over objective guidance), and \textbf{framing sycophancy} (uncritically accepting user's assumptions rather than correcting them). Each dimension is scored 1--5, with higher values indicating greater user appeasement over truthfulness. The final sycophancy score is calculated as the arithmetic mean of these three dimensions.
Judges compare model responses against English reference answers treated as factual ground truth, distinguishing between responses that correct misleading statements, partially align with truth, or fully adopt erroneous premises. Brief politeness is excluded from high sycophancy ratings, whereas repeated validation or uncritical adoption of misleading premises is penalized. A separate Qwen Instruct instance parses judge outputs, extracting the three dimension scores, computed average, and textual rationale into standardized JSON format, ensuring reproducible and interpretable evaluations.
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Quantitative Metrics}
To quantify the trade-off between social deference and factual accuracy, the system employs a custom metric engine computing two primary variables. The \textbf{Formality Ratio} measures response politeness (normalized 0.0--1.0) via rule-based detection of Japanese honorific markers: Sonkeigo verbs (e.g., \textit{meshiagaru}, \textit{zonjimasu}, 1.0), Teineigo copulas (\textit{desu/masu}, 0.6), and Tameguchi particles (\textit{daro}, \textit{jan}, 0.2). The \textbf{Factual Adherence} quantifies semantic alignment by embedding the Japanese response using Gemini Embedding 001 and calculating cosine similarity against the English ground truth embedding.
By correlating Formality Ratio with Factual Adherence, the framework identifies sycophantic drift---cases where models sacrifice factual accuracy to comply with high-context politeness constraints---enabling precise quantitative assessment of how linguistic deference influences truthfulness.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation Protocol}

\subsection{Human Evaluation of LLMs as a Judge}

To validate the efficacy of the LLM-as-a-judge framework, the automated metrics were benchmarked against human judgment, following established protocols \cite{perez2022MLevaluation}. 
This validation utilized a representative subsample of the dataset ($n=72$~responses, derived from 18 of the 60 primary questions across four distinct models), accounting for 7.5\% of the total response corpus. 
To mitigate potential systemic bias, the human evaluation was conducted by native Japanese speakers external to the research project, ensuring an objective baseline.
\\
The protocol prioritizes the alignment of evaluative trends over absolute score magnitude. 
Specifically, the framework assesses the inter-rater reliability between human judges and individual LLM judges through two statistical lenses: Spearman’s rank correlation coefficient, to evaluate the consistency of model rankings by quality, and Pearson’s correlation coefficient, to quantify the linear relationship and magnitude alignment between the scores.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Statistical Analysis and Robustness Checks}

The following table presents the statistical alignment between human evaluators and the three LLM judges regarding the \textit{framing} metric.
\begin{table}[H]
    \centering
    \small 
    \caption{Inter-rater Reliability: Spearman and Pearson Correlations for Framing}
    \label{tab:framing_correlation}
    \begin{tabularx}{\columnwidth}{l @{\extracolsep{\fill}} ccc}
        \toprule
        & \multicolumn{3}{c}{Judge model} \\
        \cmidrule(lr){2-4}
        Metric & \scriptsize Gemma 3-27B & \scriptsize Llama 3.2-3B & \scriptsize Qwen 2.5-7B \\
        \midrule
        \textbf{Pearson Linear ($r$)} & & & \\
        \quad Coeff. & \textbf{0.4539} & 0.3413 & 0.1790 \\
        \quad $p$-value & (0.0006) & (0.0115) & (0.1952) \\
        \addlinespace
        \textbf{Spearman Rank ($\rho$)} & & & \\
        \quad Coeff. & \textbf{0.4075} & 0.2986 & 0.1650 \\
        \quad $p$-value & (0.0022) & (0.0283) & (0.2332) \\
        \bottomrule
    \end{tabularx}
\end{table}

The empirical data indicates a lack of statistically significant correlation for the \textbf{Qwen 2.5-7B Instruct} model ($p > 0.19$ for both coefficients). 
Consequently, this model was excluded from subsequent analysis as its evaluative output does not reliably reflect human judgment. In contrast, both \textbf{Gemma 3-27B} and \textbf{Llama 3.2-3B Instruct} demonstrate a positive correlation with human ground truth.
While the correlation coefficients remain moderate ($r \approx 0.30\text{--}0.45$), the remarkably low $p$-values ($p < 0.05$) confirm that these associations are statistically robust and not the result of stochastic variation.
\\
The degree of alignment with human evaluators also appears to scale positively with model parameters and general reasoning capabilities.
\\
Furthermore, a significant divergence in absolute score magnitude was observed. Human evaluators assigned a mean sycophancy score of 0.80, whereas \textbf{Gemma 3-27B} yielded a mean of 2.09. 
This discrepancy suggests a fundamental difference in evaluative calibration: while the trends remain correlated, LLMs exhibit a higher sensitivity to sycophantic markers, resulting in more ``stringent'' or ``harsh'' grading behaviors compared to human subjects.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Results}

The empirical evaluation reveals a consistent relationship between grammatical politeness and sycophantic behavior, governed by both linguistic framing and model architecture. Table \ref{tab:sycophancy_scores} presents the aggregate sycophancy scores across language variants, while Table \ref{tab:model_breakdown} details the performance of individual generation models.

\subsection{Impact of Language and Register}

The transition from English to Japanese emerged as the strongest predictor of sycophantic behavior. 
The mean absolute difference between the English baseline and Japanese variants ($\Delta_{\text{EN}\rightarrow\text{JP}} = 0.47$ for Gemma) was over twice the magnitude of internal register variations, indicating that the language shift itself drives the bulk of the behavioral change.
\\
\textbf{The "Sonkeigo" Effect.}
Crucially, the hyper-formal \textbf{Sonkeigo} register consistently yielded the highest sycophancy scores across all evaluation conditions, validating the hypothesized politeness-truthfulness trade-off. 
Gemma 3-27B recorded a 42.4\% increase from the English baseline to Sonkeigo, while Llama 3.2-3B showed a 21.9\% increase. 
Most notably, human evaluators demonstrated a 45.2\% spike, confirming that excessive grammatical deference correlates strongly with increased agreement.
\\
\textbf{Non-Linearity and Calibration.}
While the extremes (English vs. Sonkeigo) followed a predictable pattern, the intermediate \textbf{Teineigo} register defied monotonic expectations. 
Both Human and Gemma 3 evaluators rated standard politeness (Teineigo) lower than casual speech (Tameguchi), with human scores dropping from 0.88 to 0.79. 
Regarding calibration, although human evaluators applied stricter standards (mean = 0.80) than LLM judges (Gemma: 1.75, Llama: 2.12), all judges preserved the fundamental ordering of English $<$ Japanese, confirming the robustness of these findings.
\begin{table}[H]
    \centering
    \small 
    \caption{Average sycophancy scores by judge and language variant}
    \label{tab:sycophancy_scores}
    \begin{tabularx}{\columnwidth}{lXXX}
        \toprule
        & \multicolumn{3}{c}{Judge model} \\
        \cmidrule(lr){2-4}
        \scriptsize Language variant & \scriptsize Human & \scriptsize Gemma \mbox{3-27B} & \scriptsize Llama 3.2-3B Instruct \\
        \midrule
            EN\_Base & 0.62 & 1.39 & 1.87 \\
            JP\_Tameguchi & 0.88 & 1.82 & 2.11 \\
            JP\_Teineigo & 0.79 & 1.79 & 2.23 \\
            JP\_Sonkeigo & \textbf{0.90} & \textbf{1.98} & \textbf{2.28} \\
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Architectural Susceptibility}

Disaggregating results by generation model reveals significant disparities in robustness (Table \ref{tab:model_breakdown}). 
\textbf{Mistral v0.3} demonstrated the highest overall susceptibility, recording an extreme peak of \textbf{2.57} in the Sonkeigo register: the highest individual score observed in our study. It also displayed the most pronounced "language gap," with scores surging by +1.04 points (1.53 $\rightarrow$ 2.57) upon shifting to Japanese.
In contrast, \textbf{Gemma 3-27B} exhibited the greatest relative robustness, maintaining consistent scores (max 1.88) across registers, suggesting that its larger parameter count or specific alignment training provides better resistance to honorific-induced bias.

\begin{table}[H]
    \centering
    \small
    \caption{Average sycophancy scores by generation model across language variants (aggregated across all judges)}
    \label{tab:model_breakdown}
    \begin{tabularx}{\columnwidth}{lXXXX}
        \toprule
        & \multicolumn{4}{c}{Generation Model} \\
        \cmidrule(lr){2-5}
        \scriptsize Variant & \scriptsize GLM-4 & \scriptsize Gemma 3 & \scriptsize Qwen 2.5 & \scriptsize Mistral \\
        \midrule
            EN\_Base      & 1.62 & 1.55 & 1.45 & 1.53 \\
            JP\_Tameguchi & \textbf{1.99} & \textbf{1.88} & 1.93 & 2.29 \\
            JP\_Teineigo  & 1.95 & 1.86 & 1.95 & 2.28 \\
            JP\_Sonkeigo  & 1.96 & \textbf{1.88} & \textbf{2.01} & \textbf{2.57} \\
        \bottomrule
    \end{tabularx}
\end{table}

\paragraph{Training Methodology and Register Sensitivity.}
These results highlight that sycophancy is an interaction between linguistic cues and model-specific priors. 
The extreme sensitivity of \textbf{Mistral v0.3} can be attributed to its training methodology: unlike Gemma 3 or Qwen 2.5, which benefit from extensive RLHF and multilingual optimization, Mistral lacks aggressive safety alignment in non-primary languages. Consequently, it likely associates high-honorific tokens with a "service persona," conflating politeness with total submission.

Conversely, \textbf{GLM-4} deviated from the standard trajectory, peaking in the casual \textit{Tameguchi} register (1.99) rather than Sonkeigo. This inverted pattern suggests that distinct training distributions can encode sycophancy in unexpected registers, independent of standard politeness hierarchies.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

This study introduces KEIGO-SYNC, a controlled evaluation framework for measuring the relationship between grammatical politeness and sycophantic behavior in multilingual large language models. 
By isolating honorific register as an independent variable while maintaining semantic invariance, we provide empirical evidence for a language-level sycophancy gap that substantially exceeds intra-register variation within Japanese.

\subsection{Principal Findings}
Our results demonstrate two primary phenomena. 
First, a consistent cross-linguistic effect emerges whereby Japanese prompts elicit systematically higher sycophancy scores than semantically equivalent English prompts, with language transition effects ($\Delta_{\text{EN}\rightarrow\text{JP}}$) approximately 2--3 times larger than maximum intra-Japanese register differences. 
Second, while Sonkeigo (honorific) register exhibited the highest sycophancy scores in all of our judges conditions, the gradation across Japanese registers (Tameguchi, Teineigo, Sonkeigo) displayed non-monotonic patterns suggesting complex interactions between grammatical formality, cultural priors embedded in training data, and model-specific alignment objectives.
This aligns with recent findings that Japanese linguistic formalities can significantly alter the quality and accuracy of AI-generated technical content \cite{matsui2024honorific}.

\subsection{Limitations and Methodological Constraints}

Several methodological limitations constrain the generalizability of these findings and warrant explicit acknowledgment.


\textbf{Model Constraints.} The LLM-as-judge evaluation relied on models of limited capacity (Gemma 3-27B, Llama 3.2-3B) due to budgetary and computational constraints.
 Ideally, frontier models such as Gemini 3 or GPT-5 would serve as judges to maximize linguistic sophistication and cross-cultural reasoning capability.
 The use of smaller open-source models, while enabling reproducibility, may have introduced evaluation noise or failed to capture subtle semantic nuances in Japanese honorific constructions.


\textbf{Dataset Scale.} The evaluation corpus comprised only 60 prompts, falling substantially short of the initially projected target of 200+ items.
 This limited sample size constrains statistical power for detecting interaction effects between register, semantic domain, and model architecture.
 Fine-grained analyses of domain-specific or fallacy-type-specific sycophancy patterns remain underpowered, and confidence intervals around effect size estimates are correspondingly wide.


\textbf{Prompt Design Uniformity.} All experimental items followed a rigid [Assertion] + [Question] template structure.
 This uniformity, while controlling for structural confounds, limits ecological validity.
 Real-world sycophancy traps exhibit substantial variation in rhetorical framing, interrogative vs. declarative phrasing, and implicit vs. explicit false premises.
 The absence of paraphrastic variation and alternative formulation strategies may have artificially stabilized model responses, potentially underestimating the true variance in sycophantic behavior across naturalistic interaction contexts.


\textbf{Translation Quality Assurance.} While all Japanese translations were produced by a native speaker, the absence of formal linguistic expertise in Japanese honorific pragmatics introduces potential validity threats.
 Subtle errors in Keigo application, inappropriate register mixing, or unnatural formality gradations may have inadvertently confounded the manipulation.
 Ideally, translations would undergo expert review by a specialist in Japanese sociolinguistics or undergo inter-rater reliability assessment by multiple native speakers with metalinguistic training.


\subsection{Future Directions}

Despite these limitations, KEIGO-SYNC establishes a replicable methodology for isolating grammatical register effects in safety-critical model behaviors.
 Future work should expand the dataset to include additional high-context languages (e.g., Korean, Thai, Javanese), increase sample sizes to enable robust statistical inference, diversify prompt structures to capture naturalistic variation, and employ frontier judge models or specialized bilingual annotators.
 Additionally, controlled ablation studies examining the interaction between RLHF intensity and honorific-induced sycophancy could clarify whether this phenomenon is intrinsic to alignment procedures or correctable through modified training objectives.
 Ultimately, addressing the politeness-truthfulness trade-off represents a necessary condition for the safe deployment of culturally-aware language models in global settings.

\section{Citations and references}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to include your bibliography file.

\bibliography{references}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
